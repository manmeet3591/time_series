{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bth5wDHQAthl"
      },
      "source": [
        "# Timeseries training using LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUoC6yxP9TQ-",
        "outputId": "31e425fd-1948-4b29-c420-f08c33e6d0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqyWv_xSELF7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxKVycfj8uRl"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/prerna_data/hadgem_aviso.nc\"\n",
        "ds   = xr.open_mfdataset(path, decode_times=False, engine=\"h5netcdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGow42qI9jQH"
      },
      "outputs": [],
      "source": [
        "ds   = ds.mean(dim=\"ensemble_member\")\n",
        "\n",
        "X_in = ds.DSL.fillna(0).values        # (T, 240, 360)#hadgem dsl array\n",
        "y_in = ds.avisodsl.fillna(0).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYFL_6eHcp5G",
        "outputId": "0dc683ea-4ae4-438e-bde8-45c1f1210823"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xarray.Dataset> Size: 365MB\n",
              "Dimensions:            (TIME: 264, LATITUDE241_480: 240, LONGITUDE841_1200: 360)\n",
              "Coordinates:\n",
              "  * TIME               (TIME) float64 2kB 5.15e+04 5.152e+04 ... 5.938e+04\n",
              "  * LATITUDE241_480    (LATITUDE241_480) float32 960B -29.88 -29.62 ... 29.88\n",
              "  * LONGITUDE841_1200  (LONGITUDE841_1200) float32 1kB 30.12 30.38 ... 119.9\n",
              "Data variables:\n",
              "    DSL                (TIME, LATITUDE241_480, LONGITUDE841_1200) float64 182MB dask.array<chunksize=(1, 120, 180), meta=np.ndarray>\n",
              "    avisodsl           (TIME, LATITUDE241_480, LONGITUDE841_1200) float64 182MB dask.array<chunksize=(1, 240, 360), meta=np.ndarray>"
            ],
            "text/html": [
              "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
              "<defs>\n",
              "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
              "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "</symbol>\n",
              "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
              "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "</symbol>\n",
              "</defs>\n",
              "</svg>\n",
              "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
              " *\n",
              " */\n",
              "\n",
              ":root {\n",
              "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
              "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
              "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
              "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
              "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
              "  --xr-background-color: var(--jp-layout-color0, white);\n",
              "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
              "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
              "}\n",
              "\n",
              "html[theme=\"dark\"],\n",
              "html[data-theme=\"dark\"],\n",
              "body[data-theme=\"dark\"],\n",
              "body.vscode-dark {\n",
              "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
              "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
              "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
              "  --xr-border-color: #1f1f1f;\n",
              "  --xr-disabled-color: #515151;\n",
              "  --xr-background-color: #111111;\n",
              "  --xr-background-color-row-even: #111111;\n",
              "  --xr-background-color-row-odd: #313131;\n",
              "}\n",
              "\n",
              ".xr-wrap {\n",
              "  display: block !important;\n",
              "  min-width: 300px;\n",
              "  max-width: 700px;\n",
              "}\n",
              "\n",
              ".xr-text-repr-fallback {\n",
              "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-header {\n",
              "  padding-top: 6px;\n",
              "  padding-bottom: 6px;\n",
              "  margin-bottom: 4px;\n",
              "  border-bottom: solid 1px var(--xr-border-color);\n",
              "}\n",
              "\n",
              ".xr-header > div,\n",
              ".xr-header > ul {\n",
              "  display: inline;\n",
              "  margin-top: 0;\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-obj-type,\n",
              ".xr-array-name {\n",
              "  margin-left: 2px;\n",
              "  margin-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-obj-type {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-sections {\n",
              "  padding-left: 0 !important;\n",
              "  display: grid;\n",
              "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
              "}\n",
              "\n",
              ".xr-section-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-section-item input {\n",
              "  display: inline-block;\n",
              "  opacity: 0;\n",
              "  height: 0;\n",
              "}\n",
              "\n",
              ".xr-section-item input + label {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label {\n",
              "  cursor: pointer;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-item input:focus + label {\n",
              "  border: 2px solid var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label:hover {\n",
              "  color: var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-summary {\n",
              "  grid-column: 1;\n",
              "  color: var(--xr-font-color2);\n",
              "  font-weight: 500;\n",
              "}\n",
              "\n",
              ".xr-section-summary > span {\n",
              "  display: inline-block;\n",
              "  padding-left: 0.5em;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in + label:before {\n",
              "  display: inline-block;\n",
              "  content: \"►\";\n",
              "  font-size: 11px;\n",
              "  width: 15px;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label:before {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label:before {\n",
              "  content: \"▼\";\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label > span {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-summary,\n",
              ".xr-section-inline-details {\n",
              "  padding-top: 4px;\n",
              "  padding-bottom: 4px;\n",
              "}\n",
              "\n",
              ".xr-section-inline-details {\n",
              "  grid-column: 2 / -1;\n",
              "}\n",
              "\n",
              ".xr-section-details {\n",
              "  display: none;\n",
              "  grid-column: 1 / -1;\n",
              "  margin-bottom: 5px;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked ~ .xr-section-details {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-array-wrap {\n",
              "  grid-column: 1 / -1;\n",
              "  display: grid;\n",
              "  grid-template-columns: 20px auto;\n",
              "}\n",
              "\n",
              ".xr-array-wrap > label {\n",
              "  grid-column: 1;\n",
              "  vertical-align: top;\n",
              "}\n",
              "\n",
              ".xr-preview {\n",
              "  color: var(--xr-font-color3);\n",
              "}\n",
              "\n",
              ".xr-array-preview,\n",
              ".xr-array-data {\n",
              "  padding: 0 5px !important;\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-array-data,\n",
              ".xr-array-in:checked ~ .xr-array-preview {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-array-in:checked ~ .xr-array-data,\n",
              ".xr-array-preview {\n",
              "  display: inline-block;\n",
              "}\n",
              "\n",
              ".xr-dim-list {\n",
              "  display: inline-block !important;\n",
              "  list-style: none;\n",
              "  padding: 0 !important;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list li {\n",
              "  display: inline-block;\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list:before {\n",
              "  content: \"(\";\n",
              "}\n",
              "\n",
              ".xr-dim-list:after {\n",
              "  content: \")\";\n",
              "}\n",
              "\n",
              ".xr-dim-list li:not(:last-child):after {\n",
              "  content: \",\";\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-has-index {\n",
              "  font-weight: bold;\n",
              "}\n",
              "\n",
              ".xr-var-list,\n",
              ".xr-var-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-var-item > div,\n",
              ".xr-var-item label,\n",
              ".xr-var-item > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-even);\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-var-item > .xr-var-name:hover span {\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-var-list > li:nth-child(odd) > div,\n",
              ".xr-var-list > li:nth-child(odd) > label,\n",
              ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-odd);\n",
              "}\n",
              "\n",
              ".xr-var-name {\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-var-dims {\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-var-dtype {\n",
              "  grid-column: 3;\n",
              "  text-align: right;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-preview {\n",
              "  grid-column: 4;\n",
              "}\n",
              "\n",
              ".xr-index-preview {\n",
              "  grid-column: 2 / 5;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-name,\n",
              ".xr-var-dims,\n",
              ".xr-var-dtype,\n",
              ".xr-preview,\n",
              ".xr-attrs dt {\n",
              "  white-space: nowrap;\n",
              "  overflow: hidden;\n",
              "  text-overflow: ellipsis;\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-var-name:hover,\n",
              ".xr-var-dims:hover,\n",
              ".xr-var-dtype:hover,\n",
              ".xr-attrs dt:hover {\n",
              "  overflow: visible;\n",
              "  width: auto;\n",
              "  z-index: 1;\n",
              "}\n",
              "\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  display: none;\n",
              "  background-color: var(--xr-background-color) !important;\n",
              "  padding-bottom: 5px !important;\n",
              "}\n",
              "\n",
              ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
              ".xr-var-data-in:checked ~ .xr-var-data,\n",
              ".xr-index-data-in:checked ~ .xr-index-data {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              ".xr-var-data > table {\n",
              "  float: right;\n",
              "}\n",
              "\n",
              ".xr-var-name span,\n",
              ".xr-var-data,\n",
              ".xr-index-name div,\n",
              ".xr-index-data,\n",
              ".xr-attrs {\n",
              "  padding-left: 25px !important;\n",
              "}\n",
              "\n",
              ".xr-attrs,\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  grid-column: 1 / -1;\n",
              "}\n",
              "\n",
              "dl.xr-attrs {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  display: grid;\n",
              "  grid-template-columns: 125px auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt,\n",
              ".xr-attrs dd {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  float: left;\n",
              "  padding-right: 10px;\n",
              "  width: auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt {\n",
              "  font-weight: normal;\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-attrs dt:hover span {\n",
              "  display: inline-block;\n",
              "  background: var(--xr-background-color);\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-attrs dd {\n",
              "  grid-column: 2;\n",
              "  white-space: pre-wrap;\n",
              "  word-break: break-all;\n",
              "}\n",
              "\n",
              ".xr-icon-database,\n",
              ".xr-icon-file-text2,\n",
              ".xr-no-icon {\n",
              "  display: inline-block;\n",
              "  vertical-align: middle;\n",
              "  width: 1em;\n",
              "  height: 1.5em !important;\n",
              "  stroke-width: 0;\n",
              "  stroke: currentColor;\n",
              "  fill: currentColor;\n",
              "}\n",
              "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 365MB\n",
              "Dimensions:            (TIME: 264, LATITUDE241_480: 240, LONGITUDE841_1200: 360)\n",
              "Coordinates:\n",
              "  * TIME               (TIME) float64 2kB 5.15e+04 5.152e+04 ... 5.938e+04\n",
              "  * LATITUDE241_480    (LATITUDE241_480) float32 960B -29.88 -29.62 ... 29.88\n",
              "  * LONGITUDE841_1200  (LONGITUDE841_1200) float32 1kB 30.12 30.38 ... 119.9\n",
              "Data variables:\n",
              "    DSL                (TIME, LATITUDE241_480, LONGITUDE841_1200) float64 182MB dask.array&lt;chunksize=(1, 120, 180), meta=np.ndarray&gt;\n",
              "    avisodsl           (TIME, LATITUDE241_480, LONGITUDE841_1200) float64 182MB dask.array&lt;chunksize=(1, 240, 360), meta=np.ndarray&gt;</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-e0022d48-a036-48c2-afe1-9fe1bad4c696' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-e0022d48-a036-48c2-afe1-9fe1bad4c696' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>TIME</span>: 264</li><li><span class='xr-has-index'>LATITUDE241_480</span>: 240</li><li><span class='xr-has-index'>LONGITUDE841_1200</span>: 360</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-3b07f9eb-3e52-4f72-841c-289ff868b560' class='xr-section-summary-in' type='checkbox'  checked><label for='section-3b07f9eb-3e52-4f72-841c-289ff868b560' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>TIME</span></div><div class='xr-var-dims'>(TIME)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>5.15e+04 5.152e+04 ... 5.938e+04</div><input id='attrs-449e526e-9793-4c6c-8d65-ad23249c15f8' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-449e526e-9793-4c6c-8d65-ad23249c15f8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8302d926-352e-4817-861d-d2db9dfb8920' class='xr-var-data-in' type='checkbox'><label for='data-8302d926-352e-4817-861d-d2db9dfb8920' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>long_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>units :</span></dt><dd>days since 1850-01-01</dd><dt><span>calendar :</span></dt><dd>360_day</dd></dl></div><div class='xr-var-data'><pre>array([51495., 51525., 51555., ..., 59325., 59355., 59385.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>LATITUDE241_480</span></div><div class='xr-var-dims'>(LATITUDE241_480)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-29.88 -29.62 ... 29.62 29.88</div><input id='attrs-f267dfa1-1c1c-434d-95fa-519616c59596' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f267dfa1-1c1c-434d-95fa-519616c59596' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-151487fc-90d5-4faa-a7e3-823b71464a75' class='xr-var-data-in' type='checkbox'><label for='data-151487fc-90d5-4faa-a7e3-823b71464a75' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-29.875, -29.625, -29.375, ...,  29.375,  29.625,  29.875],\n",
              "      dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>LONGITUDE841_1200</span></div><div class='xr-var-dims'>(LONGITUDE841_1200)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>30.12 30.38 30.62 ... 119.6 119.9</div><input id='attrs-36859785-3c64-47d8-b745-c1cf4907a803' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-36859785-3c64-47d8-b745-c1cf4907a803' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6b4c9f26-77d5-4025-9d0b-bc053f01c303' class='xr-var-data-in' type='checkbox'><label for='data-6b4c9f26-77d5-4025-9d0b-bc053f01c303' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 30.125,  30.375,  30.625, ..., 119.375, 119.625, 119.875],\n",
              "      dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-20b6ca83-c7ef-4830-a4cc-ba3bdf5e4bf1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-20b6ca83-c7ef-4830-a4cc-ba3bdf5e4bf1' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>DSL</span></div><div class='xr-var-dims'>(TIME, LATITUDE241_480, LONGITUDE841_1200)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(1, 120, 180), meta=np.ndarray&gt;</div><input id='attrs-09cd72d9-0188-43ba-b9ac-c56c9f299c8e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-09cd72d9-0188-43ba-b9ac-c56c9f299c8e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-15364bdc-a4ec-457d-a325-3254217ef8e3' class='xr-var-data-in' type='checkbox'><label for='data-15364bdc-a4ec-457d-a325-3254217ef8e3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
              "    <tr>\n",
              "        <td>\n",
              "            <table style=\"border-collapse: collapse;\">\n",
              "                <thead>\n",
              "                    <tr>\n",
              "                        <td> </td>\n",
              "                        <th> Array </th>\n",
              "                        <th> Chunk </th>\n",
              "                    </tr>\n",
              "                </thead>\n",
              "                <tbody>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Bytes </th>\n",
              "                        <td> 174.02 MiB </td>\n",
              "                        <td> 168.75 kiB </td>\n",
              "                    </tr>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Shape </th>\n",
              "                        <td> (264, 240, 360) </td>\n",
              "                        <td> (1, 120, 180) </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Dask graph </th>\n",
              "                        <td colspan=\"2\"> 1056 chunks in 4 graph layers </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Data type </th>\n",
              "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
              "                    </tr>\n",
              "                </tbody>\n",
              "            </table>\n",
              "        </td>\n",
              "        <td>\n",
              "        <svg width=\"231\" height=\"181\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"61\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"10\" y1=\"40\" x2=\"61\" y2=\"91\" />\n",
              "  <line x1=\"10\" y1=\"80\" x2=\"61\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"80\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"82\" />\n",
              "  <line x1=\"15\" y1=\"5\" x2=\"15\" y2=\"85\" />\n",
              "  <line x1=\"18\" y1=\"8\" x2=\"18\" y2=\"88\" />\n",
              "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"90\" />\n",
              "  <line x1=\"23\" y1=\"13\" x2=\"23\" y2=\"93\" />\n",
              "  <line x1=\"26\" y1=\"16\" x2=\"26\" y2=\"96\" />\n",
              "  <line x1=\"29\" y1=\"19\" x2=\"29\" y2=\"99\" />\n",
              "  <line x1=\"31\" y1=\"21\" x2=\"31\" y2=\"101\" />\n",
              "  <line x1=\"34\" y1=\"24\" x2=\"34\" y2=\"104\" />\n",
              "  <line x1=\"37\" y1=\"27\" x2=\"37\" y2=\"107\" />\n",
              "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"109\" />\n",
              "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"112\" />\n",
              "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"115\" />\n",
              "  <line x1=\"48\" y1=\"38\" x2=\"48\" y2=\"118\" />\n",
              "  <line x1=\"50\" y1=\"40\" x2=\"50\" y2=\"120\" />\n",
              "  <line x1=\"53\" y1=\"43\" x2=\"53\" y2=\"123\" />\n",
              "  <line x1=\"56\" y1=\"46\" x2=\"56\" y2=\"126\" />\n",
              "  <line x1=\"59\" y1=\"49\" x2=\"59\" y2=\"129\" />\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"10.0,0.0 61.76470588235294,51.76470588235294 61.76470588235294,131.76470588235293 10.0,80.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"12\" y1=\"2\" x2=\"132\" y2=\"2\" />\n",
              "  <line x1=\"15\" y1=\"5\" x2=\"135\" y2=\"5\" />\n",
              "  <line x1=\"18\" y1=\"8\" x2=\"138\" y2=\"8\" />\n",
              "  <line x1=\"20\" y1=\"10\" x2=\"140\" y2=\"10\" />\n",
              "  <line x1=\"23\" y1=\"13\" x2=\"143\" y2=\"13\" />\n",
              "  <line x1=\"26\" y1=\"16\" x2=\"146\" y2=\"16\" />\n",
              "  <line x1=\"29\" y1=\"19\" x2=\"149\" y2=\"19\" />\n",
              "  <line x1=\"31\" y1=\"21\" x2=\"151\" y2=\"21\" />\n",
              "  <line x1=\"34\" y1=\"24\" x2=\"154\" y2=\"24\" />\n",
              "  <line x1=\"37\" y1=\"27\" x2=\"157\" y2=\"27\" />\n",
              "  <line x1=\"39\" y1=\"29\" x2=\"159\" y2=\"29\" />\n",
              "  <line x1=\"42\" y1=\"32\" x2=\"162\" y2=\"32\" />\n",
              "  <line x1=\"45\" y1=\"35\" x2=\"165\" y2=\"35\" />\n",
              "  <line x1=\"48\" y1=\"38\" x2=\"168\" y2=\"38\" />\n",
              "  <line x1=\"50\" y1=\"40\" x2=\"170\" y2=\"40\" />\n",
              "  <line x1=\"53\" y1=\"43\" x2=\"173\" y2=\"43\" />\n",
              "  <line x1=\"56\" y1=\"46\" x2=\"176\" y2=\"46\" />\n",
              "  <line x1=\"59\" y1=\"49\" x2=\"179\" y2=\"49\" />\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"181\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"61\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"70\" y1=\"0\" x2=\"121\" y2=\"51\" />\n",
              "  <line x1=\"130\" y1=\"0\" x2=\"181\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"10.0,0.0 130.0,0.0 181.76470588235293,51.76470588235294 61.76470588235294,51.76470588235294\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"181\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"61\" y1=\"91\" x2=\"181\" y2=\"91\" />\n",
              "  <line x1=\"61\" y1=\"131\" x2=\"181\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"121\" y1=\"51\" x2=\"121\" y2=\"131\" />\n",
              "  <line x1=\"181\" y1=\"51\" x2=\"181\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"61.76470588235294,51.76470588235294 181.76470588235293,51.76470588235294 181.76470588235293,131.76470588235293 61.76470588235294,131.76470588235293\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Text -->\n",
              "  <text x=\"121.764706\" y=\"151.764706\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >360</text>\n",
              "  <text x=\"201.764706\" y=\"91.764706\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,201.764706,91.764706)\">240</text>\n",
              "  <text x=\"25.882353\" y=\"125.882353\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,25.882353,125.882353)\">264</text>\n",
              "</svg>\n",
              "        </td>\n",
              "    </tr>\n",
              "</table></div></li><li class='xr-var-item'><div class='xr-var-name'><span>avisodsl</span></div><div class='xr-var-dims'>(TIME, LATITUDE241_480, LONGITUDE841_1200)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(1, 240, 360), meta=np.ndarray&gt;</div><input id='attrs-01eb381f-c69d-4bc6-a1e7-44df33f270f3' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-01eb381f-c69d-4bc6-a1e7-44df33f270f3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-065ddda8-2ec0-487c-ba4d-3d555f58ffa4' class='xr-var-data-in' type='checkbox'><label for='data-065ddda8-2ec0-487c-ba4d-3d555f58ffa4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
              "    <tr>\n",
              "        <td>\n",
              "            <table style=\"border-collapse: collapse;\">\n",
              "                <thead>\n",
              "                    <tr>\n",
              "                        <td> </td>\n",
              "                        <th> Array </th>\n",
              "                        <th> Chunk </th>\n",
              "                    </tr>\n",
              "                </thead>\n",
              "                <tbody>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Bytes </th>\n",
              "                        <td> 174.02 MiB </td>\n",
              "                        <td> 675.00 kiB </td>\n",
              "                    </tr>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Shape </th>\n",
              "                        <td> (264, 240, 360) </td>\n",
              "                        <td> (1, 240, 360) </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Dask graph </th>\n",
              "                        <td colspan=\"2\"> 264 chunks in 2 graph layers </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Data type </th>\n",
              "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
              "                    </tr>\n",
              "                </tbody>\n",
              "            </table>\n",
              "        </td>\n",
              "        <td>\n",
              "        <svg width=\"231\" height=\"181\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"61\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"10\" y1=\"80\" x2=\"61\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"80\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"82\" />\n",
              "  <line x1=\"15\" y1=\"5\" x2=\"15\" y2=\"85\" />\n",
              "  <line x1=\"18\" y1=\"8\" x2=\"18\" y2=\"88\" />\n",
              "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"90\" />\n",
              "  <line x1=\"23\" y1=\"13\" x2=\"23\" y2=\"93\" />\n",
              "  <line x1=\"26\" y1=\"16\" x2=\"26\" y2=\"96\" />\n",
              "  <line x1=\"29\" y1=\"19\" x2=\"29\" y2=\"99\" />\n",
              "  <line x1=\"31\" y1=\"21\" x2=\"31\" y2=\"101\" />\n",
              "  <line x1=\"34\" y1=\"24\" x2=\"34\" y2=\"104\" />\n",
              "  <line x1=\"37\" y1=\"27\" x2=\"37\" y2=\"107\" />\n",
              "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"109\" />\n",
              "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"112\" />\n",
              "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"115\" />\n",
              "  <line x1=\"48\" y1=\"38\" x2=\"48\" y2=\"118\" />\n",
              "  <line x1=\"50\" y1=\"40\" x2=\"50\" y2=\"120\" />\n",
              "  <line x1=\"53\" y1=\"43\" x2=\"53\" y2=\"123\" />\n",
              "  <line x1=\"56\" y1=\"46\" x2=\"56\" y2=\"126\" />\n",
              "  <line x1=\"59\" y1=\"49\" x2=\"59\" y2=\"129\" />\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"10.0,0.0 61.76470588235294,51.76470588235294 61.76470588235294,131.76470588235293 10.0,80.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"12\" y1=\"2\" x2=\"132\" y2=\"2\" />\n",
              "  <line x1=\"15\" y1=\"5\" x2=\"135\" y2=\"5\" />\n",
              "  <line x1=\"18\" y1=\"8\" x2=\"138\" y2=\"8\" />\n",
              "  <line x1=\"20\" y1=\"10\" x2=\"140\" y2=\"10\" />\n",
              "  <line x1=\"23\" y1=\"13\" x2=\"143\" y2=\"13\" />\n",
              "  <line x1=\"26\" y1=\"16\" x2=\"146\" y2=\"16\" />\n",
              "  <line x1=\"29\" y1=\"19\" x2=\"149\" y2=\"19\" />\n",
              "  <line x1=\"31\" y1=\"21\" x2=\"151\" y2=\"21\" />\n",
              "  <line x1=\"34\" y1=\"24\" x2=\"154\" y2=\"24\" />\n",
              "  <line x1=\"37\" y1=\"27\" x2=\"157\" y2=\"27\" />\n",
              "  <line x1=\"39\" y1=\"29\" x2=\"159\" y2=\"29\" />\n",
              "  <line x1=\"42\" y1=\"32\" x2=\"162\" y2=\"32\" />\n",
              "  <line x1=\"45\" y1=\"35\" x2=\"165\" y2=\"35\" />\n",
              "  <line x1=\"48\" y1=\"38\" x2=\"168\" y2=\"38\" />\n",
              "  <line x1=\"50\" y1=\"40\" x2=\"170\" y2=\"40\" />\n",
              "  <line x1=\"53\" y1=\"43\" x2=\"173\" y2=\"43\" />\n",
              "  <line x1=\"56\" y1=\"46\" x2=\"176\" y2=\"46\" />\n",
              "  <line x1=\"59\" y1=\"49\" x2=\"179\" y2=\"49\" />\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"181\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"10\" y1=\"0\" x2=\"61\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"130\" y1=\"0\" x2=\"181\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"10.0,0.0 130.0,0.0 181.76470588235293,51.76470588235294 61.76470588235294,51.76470588235294\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"181\" y2=\"51\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"61\" y1=\"131\" x2=\"181\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"181\" y1=\"51\" x2=\"181\" y2=\"131\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"61.76470588235294,51.76470588235294 181.76470588235293,51.76470588235294 181.76470588235293,131.76470588235293 61.76470588235294,131.76470588235293\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Text -->\n",
              "  <text x=\"121.764706\" y=\"151.764706\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >360</text>\n",
              "  <text x=\"201.764706\" y=\"91.764706\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,201.764706,91.764706)\">240</text>\n",
              "  <text x=\"25.882353\" y=\"125.882353\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,25.882353,125.882353)\">264</text>\n",
              "</svg>\n",
              "        </td>\n",
              "    </tr>\n",
              "</table></div></li></ul></div></li><li class='xr-section-item'><input id='section-4507e917-74b8-4a39-aeed-20970da62bcc' class='xr-section-summary-in' type='checkbox'  ><label for='section-4507e917-74b8-4a39-aeed-20970da62bcc' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>TIME</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-a9c3151d-b41c-4100-a2d8-d7ae312b992a' class='xr-index-data-in' type='checkbox'/><label for='index-a9c3151d-b41c-4100-a2d8-d7ae312b992a' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([51495.0, 51525.0, 51555.0, 51585.0, 51615.0, 51645.0, 51675.0, 51705.0,\n",
              "       51735.0, 51765.0,\n",
              "       ...\n",
              "       59115.0, 59145.0, 59175.0, 59205.0, 59235.0, 59265.0, 59295.0, 59325.0,\n",
              "       59355.0, 59385.0],\n",
              "      dtype=&#x27;float64&#x27;, name=&#x27;TIME&#x27;, length=264))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>LATITUDE241_480</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-d7e4e6c2-7de3-47a7-93d2-85079c462324' class='xr-index-data-in' type='checkbox'/><label for='index-d7e4e6c2-7de3-47a7-93d2-85079c462324' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-29.875, -29.625, -29.375, -29.125, -28.875, -28.625, -28.375, -28.125,\n",
              "       -27.875, -27.625,\n",
              "       ...\n",
              "        27.625,  27.875,  28.125,  28.375,  28.625,  28.875,  29.125,  29.375,\n",
              "        29.625,  29.875],\n",
              "      dtype=&#x27;float32&#x27;, name=&#x27;LATITUDE241_480&#x27;, length=240))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>LONGITUDE841_1200</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-31d0753c-0ab5-417e-89e8-5ef9d9b590af' class='xr-index-data-in' type='checkbox'/><label for='index-31d0753c-0ab5-417e-89e8-5ef9d9b590af' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ 30.125,  30.375,  30.625,  30.875,  31.125,  31.375,  31.625,  31.875,\n",
              "        32.125,  32.375,\n",
              "       ...\n",
              "       117.625, 117.875, 118.125, 118.375, 118.625, 118.875, 119.125, 119.375,\n",
              "       119.625, 119.875],\n",
              "      dtype=&#x27;float32&#x27;, name=&#x27;LONGITUDE841_1200&#x27;, length=360))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-09ab0cc4-a924-4256-9f6f-ad92dea2ea2a' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-09ab0cc4-a924-4256-9f6f-ad92dea2ea2a' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euVB9r05bsLo",
        "outputId": "55541284-9cf4-4c2a-d74f-10ecfd5bff33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((264, 240, 360), (264, 240, 360))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_in.shape, y_in.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vLsARVx9N0b",
        "outputId": "4fb02ce0-da40-48da-f259-60d7fce38543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full cube : T=264, H=240, W=360\n"
          ]
        }
      ],
      "source": [
        "T, H, W = X_in.shape\n",
        "print(f\"Full cube : T={T}, H={H}, W={W}\")#t is time steps, lat, long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oX6J0xnJdVs"
      },
      "outputs": [],
      "source": [
        "X_in = X_in.reshape(T, -1)\n",
        "y_in = y_in.reshape(T, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB5MSpRfcWhw",
        "outputId": "0f543ca1-3887-48df-dfd7-991df64f4500"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((264, 86400), (264, 86400))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X_in.shape, y_in.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orVZeTZpJ4Km"
      },
      "outputs": [],
      "source": [
        "X_grid = X_in.T     # shape: (H*W, T)\n",
        "y_grid = y_in.T     # shape: (H*W, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSAulOZtKUKe",
        "outputId": "ee19e6f0-acc4-4531-b370-0867e1264f67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(86400, 264)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y_grid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Uvzf459fbo"
      },
      "outputs": [],
      "source": [
        "n_pixels=X_grid.shape[0]\n",
        "pix_ids      = np.arange(n_pixels)                 # 0 … 86 399\n",
        "rng          = np.random.default_rng(42)        # reproducible\n",
        "rng.shuffle(pix_ids) # permutes the list in‑place; every pixel gets a new random position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D65HDc5s9onb"
      },
      "outputs": [],
      "source": [
        "n_train      = int(0.80 * n_pixels)\n",
        "n_val        = int(0.10 * n_pixels)\n",
        "\n",
        "train_ids    = pix_ids[:n_train]\n",
        "val_ids      = pix_ids[n_train:n_train+n_val]\n",
        "test_ids     = pix_ids[n_train+n_val:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqYtJryALusH"
      },
      "outputs": [],
      "source": [
        "X_train = X_grid[train_ids]   # shape: (n_train, T)\n",
        "y_train = y_grid[train_ids]\n",
        "\n",
        "X_val = X_grid[val_ids]\n",
        "y_val = y_grid[val_ids]\n",
        "\n",
        "X_test = X_grid[test_ids]\n",
        "y_test = y_grid[test_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj1fB4pB9wLd",
        "outputId": "b30a5fbb-6260-4bc0-beed-1711f069f0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pixel‑only cubes  :\n",
            "train  (69120, 264)\n",
            "val    (8640, 264)\n",
            "test   (8640, 264)\n"
          ]
        }
      ],
      "source": [
        "print(\"Pixel‑only cubes  :\")\n",
        "for name, arr in [(\"train\", X_train), (\"val\", X_val), (\"test\", X_test)]:\n",
        "    print(f\"{name:5s}  {arr.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ew0b8VCdFJ5",
        "outputId": "9a474d64-0cc6-4107-a4c8-db6f7caaaa7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69120, 264)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYOifl3ldGwO",
        "outputId": "3614a2b7-fc9e-4a20-de86-ed02276cb9af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69120, 264)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMwmtFlKdQi_",
        "outputId": "482e9b22-0991-477b-d264-6d2593f4f5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define and compile model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(10, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(1e-4), loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dERytIyYdQgr"
      },
      "outputs": [],
      "source": [
        "# Directory to save best models\n",
        "save_dir = '/home/cccr_rnd/arshmehar/models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Modify the checkpoint callback to include formatted filename\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    filepath=os.path.join(save_dir, \"best_model_epoch{epoch:04d}_val{val_loss:.6f}.h5\"),  # Keep your existing format\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Custom callback to print a message when training completes\n",
        "class TrainingCompleteCallback(Callback):\n",
        "    def on_train_end(self, logs=None):\n",
        "        print(\"✅ Training completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMQbfsRFdQeF",
        "outputId": "72ec376c-5e88-4373-cc8f-807bf4ecb924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0150\n",
            "Epoch 1: val_loss improved from inf to 0.01157, saving model to /home/cccr_rnd/arshmehar/models/best_model_epoch0001_val0.011572.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - loss: 0.0149 - val_loss: 0.0116\n",
            "Epoch 2/2\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0104\n",
            "Epoch 2: val_loss improved from 0.01157 to 0.00838, saving model to /home/cccr_rnd/arshmehar/models/best_model_epoch0002_val0.008379.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0104 - val_loss: 0.0084\n",
            "✅ Training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=2,\n",
        "    batch_size=2048,  # You can tune this based on memory\n",
        "    callbacks=[checkpoint_cb, TrainingCompleteCallback()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kSBToRBdQbk",
        "outputId": "8e043daf-27d0-471e-e091-0c4a2029a661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0084\n",
            "Loss: 0.008374874480068684\n"
          ]
        }
      ],
      "source": [
        "loss = model.evaluate(X_test, y_test, batch_size=2048)\n",
        "print(f\"Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCeFAoPvdbBK",
        "outputId": "75510510-933b-4ac3-fb87-2938eae72bd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8640, 264)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDg6h5gqfCDx"
      },
      "source": [
        "# PatchTST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfDCKgChfK2e"
      },
      "outputs": [],
      "source": [
        "__all__ = ['Transpose', 'get_activation_fn', 'moving_avg', 'series_decomp', 'PositionalEncoding', 'SinCosPosEncoding', 'Coord2dPosEncoding', 'Coord1dPosEncoding', 'positional_encoding']\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "    def __init__(self, *dims, contiguous=False):\n",
        "        super().__init__()\n",
        "        self.dims, self.contiguous = dims, contiguous\n",
        "    def forward(self, x):\n",
        "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
        "        else: return x.transpose(*self.dims)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation):\n",
        "    if callable(activation): return activation()\n",
        "    elif activation.lower() == \"relu\": return nn.ReLU()\n",
        "    elif activation.lower() == \"gelu\": return nn.GELU()\n",
        "    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable')\n",
        "\n",
        "\n",
        "# decomposition\n",
        "\n",
        "class moving_avg(nn.Module):\n",
        "    \"\"\"\n",
        "    Moving average block to highlight the trend of time series\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(moving_avg, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size):\n",
        "        super(series_decomp, self).__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "\n",
        "\n",
        "# pos_encoding\n",
        "\n",
        "def PositionalEncoding(q_len, d_model, normalize=True):\n",
        "    pe = torch.zeros(q_len, d_model)\n",
        "    position = torch.arange(0, q_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    if normalize:\n",
        "        pe = pe - pe.mean()\n",
        "        pe = pe / (pe.std() * 10)\n",
        "    return pe\n",
        "\n",
        "SinCosPosEncoding = PositionalEncoding\n",
        "\n",
        "def Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n",
        "    x = .5 if exponential else 1\n",
        "    i = 0\n",
        "    for i in range(100):\n",
        "        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n",
        "        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n",
        "        if abs(cpe.mean()) <= eps: break\n",
        "        elif cpe.mean() > eps: x += .001\n",
        "        else: x -= .001\n",
        "        i += 1\n",
        "    if normalize:\n",
        "        cpe = cpe - cpe.mean()\n",
        "        cpe = cpe / (cpe.std() * 10)\n",
        "    return cpe\n",
        "\n",
        "def Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n",
        "    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n",
        "    if normalize:\n",
        "        cpe = cpe - cpe.mean()\n",
        "        cpe = cpe / (cpe.std() * 10)\n",
        "    return cpe\n",
        "\n",
        "def positional_encoding(pe, learn_pe, q_len, d_model):\n",
        "    # Positional encoding\n",
        "    if pe == None:\n",
        "        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "        learn_pe = False\n",
        "    elif pe == 'zero':\n",
        "        W_pos = torch.empty((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == 'zeros':\n",
        "        W_pos = torch.empty((q_len, d_model))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == 'normal' or pe == 'gauss':\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
        "    elif pe == 'uniform':\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
        "    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n",
        "    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n",
        "    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n",
        "    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n",
        "    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n",
        "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n",
        "        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n",
        "    return nn.Parameter(W_pos, requires_grad=learn_pe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFkMcx3qfS89"
      },
      "outputs": [],
      "source": [
        "# code from https://github.com/ts-kim/RevIN, with minor modifications\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RevIN(nn.Module):\n",
        "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
        "        \"\"\"\n",
        "        :param num_features: the number of features or channels\n",
        "        :param eps: a value added for numerical stability\n",
        "        :param affine: if True, RevIN has learnable affine parameters\n",
        "        \"\"\"\n",
        "        super(RevIN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode:str):\n",
        "        if mode == 'norm':\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == 'denorm':\n",
        "            x = self._denormalize(x)\n",
        "        else: raise NotImplementedError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        # initialize RevIN params: (C,)\n",
        "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim-1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:,-1,:].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps*self.eps)\n",
        "        x = x * self.stdev\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last\n",
        "        else:\n",
        "            x = x + self.mean\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUZ1HrVJfDkN"
      },
      "outputs": [],
      "source": [
        "__all__ = ['PatchTST_backbone']\n",
        "\n",
        "# Cell\n",
        "from typing import Callable, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#from collections import OrderedDict\n",
        "# from layers.PatchTST_layers import *\n",
        "# from layers.RevIN import RevIN\n",
        "\n",
        "# Cell\n",
        "class PatchTST_backbone(nn.Module):\n",
        "    def __init__(self, c_in:int, context_window:int, target_window:int, patch_len:int, stride:int, max_seq_len:Optional[int]=1024,\n",
        "                 n_layers:int=3, d_model=128, n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None,\n",
        "                 d_ff:int=256, norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", key_padding_mask:bool='auto',\n",
        "                 padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n",
        "                 pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., head_dropout = 0, padding_patch = None,\n",
        "                 pretrain_head:bool=False, head_type = 'flatten', individual = False, revin = True, affine = True, subtract_last = False,\n",
        "                 verbose:bool=False, **kwargs):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # RevIn\n",
        "        self.revin = revin\n",
        "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
        "\n",
        "        # Patching\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        patch_num = int((context_window - patch_len)/stride + 1)\n",
        "        if padding_patch == 'end': # can be modified to general case\n",
        "            self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "            patch_num += 1\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
        "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                attn_dropout=attn_dropout, dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
        "                                pe=pe, learn_pe=learn_pe, verbose=verbose, **kwargs)\n",
        "\n",
        "        # Head\n",
        "        self.head_nf = d_model * patch_num\n",
        "        self.n_vars = c_in\n",
        "        self.pretrain_head = pretrain_head\n",
        "        self.head_type = head_type\n",
        "        self.individual = individual\n",
        "\n",
        "        if self.pretrain_head:\n",
        "            self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) # custom head passed as a partial func with all its kwargs\n",
        "        elif head_type == 'flatten':\n",
        "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, z):                                                                   # z: [bs x nvars x seq_len]\n",
        "        # norm\n",
        "        if self.revin:\n",
        "            z = z.permute(0,2,1)\n",
        "            z = self.revin_layer(z, 'norm')\n",
        "            z = z.permute(0,2,1)\n",
        "\n",
        "        # do patching\n",
        "        if self.padding_patch == 'end':\n",
        "            z = self.padding_patch_layer(z)\n",
        "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
        "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        # model\n",
        "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
        "        z = self.head(z)                                                                    # z: [bs x nvars x target_window]\n",
        "\n",
        "        # denorm\n",
        "        if self.revin:\n",
        "            z = z.permute(0,2,1)\n",
        "            z = self.revin_layer(z, 'denorm')\n",
        "            z = z.permute(0,2,1)\n",
        "        return z\n",
        "\n",
        "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
        "        return nn.Sequential(nn.Dropout(dropout),\n",
        "                    nn.Conv1d(head_nf, vars, 1)\n",
        "                    )\n",
        "\n",
        "\n",
        "class Flatten_Head(nn.Module):\n",
        "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.individual = individual\n",
        "        self.n_vars = n_vars\n",
        "\n",
        "        if self.individual:\n",
        "            self.linears = nn.ModuleList()\n",
        "            self.dropouts = nn.ModuleList()\n",
        "            self.flattens = nn.ModuleList()\n",
        "            for i in range(self.n_vars):\n",
        "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
        "                self.linears.append(nn.Linear(nf, target_window))\n",
        "                self.dropouts.append(nn.Dropout(head_dropout))\n",
        "        else:\n",
        "            self.flatten = nn.Flatten(start_dim=-2)\n",
        "            self.linear = nn.Linear(nf, target_window)\n",
        "            self.dropout = nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
        "        if self.individual:\n",
        "            x_out = []\n",
        "            for i in range(self.n_vars):\n",
        "                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n",
        "                z = self.linears[i](z)                    # z: [bs x target_window]\n",
        "                z = self.dropouts[i](z)\n",
        "                x_out.append(z)\n",
        "            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n",
        "        else:\n",
        "            x = self.flatten(x)\n",
        "            x = self.linear(x)\n",
        "            x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TSTiEncoder(nn.Module):  #i means channel-independent\n",
        "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
        "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
        "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
        "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "\n",
        "        # Input encoding\n",
        "        q_len = patch_num\n",
        "        self.W_P = nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
        "        self.seq_len = q_len\n",
        "\n",
        "        # Positional encoding\n",
        "        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n",
        "\n",
        "        # Residual dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
        "\n",
        "\n",
        "    def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num]\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "        # Input encoding\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
        "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
        "\n",
        "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
        "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
        "\n",
        "        # Encoder\n",
        "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
        "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
        "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# Cell\n",
        "class TSTEncoder(nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None,\n",
        "                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n",
        "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
        "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                                      activation=activation, res_attention=res_attention,\n",
        "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
        "        self.res_attention = res_attention\n",
        "\n",
        "    def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
        "        output = src\n",
        "        scores = None\n",
        "        if self.res_attention:\n",
        "            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "        else:\n",
        "            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "class TSTEncoderLayer(nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
        "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n",
        "        super().__init__()\n",
        "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        # Multi-Head attention\n",
        "        self.res_attention = res_attention\n",
        "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_attn = nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_attn = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed-Forward\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
        "                                get_activation_fn(activation),\n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_ffn = nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_ffn = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.store_attn = store_attn\n",
        "\n",
        "\n",
        "    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None) -> Tensor:\n",
        "\n",
        "        # Multi-Head attention sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "        ## Multi-Head attention\n",
        "        if self.res_attention:\n",
        "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        if self.store_attn:\n",
        "            self.attn = attn\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "        ## Position-wise Feed-Forward\n",
        "        src2 = self.ff(src)\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return src, scores\n",
        "        else:\n",
        "            return src\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class _MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
        "        \"\"\"Multi Head Attention Layer\n",
        "        Input shape:\n",
        "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
        "            K, V:    [batch_size (bs) x q_len x d_model]\n",
        "            mask:    [q_len x q_len]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "        # Scaled Dot-Product Attention (multiple heads)\n",
        "        self.res_attention = res_attention\n",
        "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
        "\n",
        "        # Poject output\n",
        "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
        "\n",
        "\n",
        "    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None,\n",
        "                key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
        "\n",
        "        bs = Q.size(0)\n",
        "        if K is None: K = Q\n",
        "        if V is None: V = Q\n",
        "\n",
        "        # Linear (+ split in multiple heads)\n",
        "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
        "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
        "\n",
        "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "        if self.res_attention:\n",
        "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        else:\n",
        "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # back to the original inputs dimensions\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
        "        output = self.to_out(output)\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights\n",
        "\n",
        "\n",
        "class _ScaledDotProductAttention(nn.Module):\n",
        "    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
        "    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
        "    by Lee et al, 2021)\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.res_attention = res_attention\n",
        "        head_dim = d_model // n_heads\n",
        "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
        "        self.lsa = lsa\n",
        "\n",
        "    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
        "        '''\n",
        "        Input shape:\n",
        "            q               : [bs x n_heads x max_q_len x d_k]\n",
        "            k               : [bs x n_heads x d_k x seq_len]\n",
        "            v               : [bs x n_heads x seq_len x d_v]\n",
        "            prev            : [bs x n_heads x q_len x seq_len]\n",
        "            key_padding_mask: [bs x seq_len]\n",
        "            attn_mask       : [1 x seq_len x seq_len]\n",
        "        Output shape:\n",
        "            output:  [bs x n_heads x q_len x d_v]\n",
        "            attn   : [bs x n_heads x q_len x seq_len]\n",
        "            scores : [bs x n_heads x q_len x seq_len]\n",
        "        '''\n",
        "\n",
        "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # Add pre-softmax attention scores from the previous layer (optional)\n",
        "        if prev is not None: attn_scores = attn_scores + prev\n",
        "\n",
        "        # Attention mask (optional)\n",
        "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
        "            else:\n",
        "                attn_scores += attn_mask\n",
        "\n",
        "        # Key padding mask (optional)\n",
        "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
        "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
        "\n",
        "        # normalize the attention weights\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # compute the new values given the attention weights\n",
        "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
        "\n",
        "        if self.res_attention: return output, attn_weights, attn_scores\n",
        "        else: return output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg9RdnpWfDhe"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPyT-0LbfDfR"
      },
      "outputs": [],
      "source": [
        "class Configs:\n",
        "    def __init__(self):\n",
        "        self.enc_in = 1             # Number of features\n",
        "        self.seq_len = 264           # Input sequence length\n",
        "        self.pred_len = 264          # Target sequence length (same as input for many-to-many)\n",
        "        self.e_layers = 2           # Number of transformer encoder layers\n",
        "        self.n_heads = 4            # Number of attention heads\n",
        "        self.d_model = 64           # Embedding dimension\n",
        "        self.d_ff = 128             # Feedforward dimension\n",
        "        self.dropout = 0.1\n",
        "        self.fc_dropout = 0.1\n",
        "        self.head_dropout = 0.1\n",
        "        self.individual = False     # Shared head for all variables\n",
        "        self.patch_len = 16         # Length of each patch\n",
        "        self.stride = 8             # Stride for patching\n",
        "        self.padding_patch = 'end'  # Padding option\n",
        "        self.revin = True           # Whether to use RevIN\n",
        "        self.affine = True\n",
        "        self.subtract_last = False\n",
        "        self.decomposition = False  # Whether to use decomposition module\n",
        "        self.kernel_size = 25       # Only used if decomposition=True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rYk5vTuhh5B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KV-xM5cj5QP"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0EQ-tK8iBAJ",
        "outputId": "97afd27c-c2b6-4275-fcd9-8b9e5debbdf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Training Loss: 0.002093\n",
            "Epoch 2 Training Loss: 0.001129\n",
            "Validation Loss: 0.000902\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert numpy arrays to torch tensors and reshape: (N, seq_len) -> (N, features=1, seq_len)\n",
        "X_train_tensor = torch.from_numpy(X_train).float().unsqueeze(1).to(device)  # shape (N,1,seq_len)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\n",
        "X_val_tensor = torch.from_numpy(X_val).float().unsqueeze(1).to(device)\n",
        "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(1).to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Instantiate model\n",
        "# configs = Configs()\n",
        "# model = PatchTST_backbone(configs).to(device)\n",
        "configs = Configs()\n",
        "model = PatchTST_backbone(\n",
        "    configs=configs,\n",
        "    c_in=configs.enc_in,\n",
        "    context_window=configs.seq_len,\n",
        "    target_window=configs.pred_len,\n",
        "    patch_len=configs.patch_len,\n",
        "    stride=configs.stride\n",
        ").to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)  # model expects input shape (batch, features, seq_len)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {running_loss / len(train_loader):.6f}\")\n",
        "\n",
        "# Validation loop example\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        val_loss += loss.item()\n",
        "print(f\"Validation Loss: {val_loss / len(val_loader):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58y2h2oReqxA",
        "outputId": "7aea7a67-78e6-42fc-801f-d49e9d64f970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(69120, 264) (69120, 264)\n"
          ]
        }
      ],
      "source": [
        "print(type(X_train), type(y_train))\n",
        "print(X_train.shape, y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4JcDZ7WP0Uv"
      },
      "source": [
        "# AUTOFORMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlhzjUCOP5LM"
      },
      "outputs": [],
      "source": [
        "# class AutoformerConfigs:\n",
        "#     def __init__(self):\n",
        "#         self.seq_len = 264\n",
        "#         self.label_len = 64\n",
        "#         self.pred_len = 264\n",
        "#         self.enc_in = 1       # number of input features\n",
        "#         self.dec_in = 1\n",
        "#         self.c_out = 1        # output feature dimension\n",
        "#         self.d_model = 64\n",
        "#         self.n_heads = 4\n",
        "#         self.d_ff = 128\n",
        "#         self.e_layers = 2\n",
        "#         self.d_layers = 1\n",
        "#         self.dropout = 0.1\n",
        "#         self.embed = 'timeF'\n",
        "#         self.freq = 'h'\n",
        "#         self.activation = 'gelu'\n",
        "#         self.output_attention = False\n",
        "#         self.moving_avg = 25\n",
        "#         self.embed_type = 0\n",
        "#         self.factor = 3\n",
        "\n",
        "class AutoformerConfigs:\n",
        "    def __init__(self):\n",
        "        self.seq_len = 264         # Full input sequence length (HadGEM)\n",
        "        self.label_len = 264       # Decoder uses full sequence for teacher forcing\n",
        "        self.pred_len = 264        # Predict full AVISO sequence\n",
        "        self.enc_in = 1            # One input feature (DSL)\n",
        "        self.dec_in = 1            # One decoder input feature\n",
        "        self.c_out = 1             # One output feature\n",
        "\n",
        "        # Transformer architecture\n",
        "        self.d_model = 64          # Embedding dimension\n",
        "        self.n_heads = 4           # Number of attention heads\n",
        "        self.d_ff = 128            # Feedforward dimension\n",
        "        self.e_layers = 2          # Encoder layers\n",
        "        self.d_layers = 1          # Decoder layers\n",
        "        self.dropout = 0.1         # General dropout\n",
        "\n",
        "        # Embedding settings\n",
        "        self.embed = 'fixed'       # No time features used — use fixed or 'learned'\n",
        "        self.freq = 'm'            # Monthly frequency (irrelevant here since no time embeddings)\n",
        "        self.embed_type = 0        # Default positional encoding\n",
        "\n",
        "        # Model behavior\n",
        "        self.activation = 'gelu'   # Activation function\n",
        "        self.output_attention = False  # Don’t return attention maps\n",
        "        self.moving_avg = 25       # Moving average for decomposition\n",
        "        self.factor = 3            # Attention sparsity factor\n",
        "# class AutoformerConfigs:\n",
        "#     def __init__(self):\n",
        "#         self.seq_len = 264\n",
        "#         self.label_len = 264\n",
        "#         self.pred_len = 264\n",
        "#         self.enc_in = 1\n",
        "#         self.dec_in = 1\n",
        "#         self.c_out = 1\n",
        "#         self.d_model = 64\n",
        "#         self.n_heads = 4\n",
        "#         self.d_ff = 128\n",
        "#         self.e_layers = 2\n",
        "#         self.d_layers = 1\n",
        "#         self.dropout = 0.1\n",
        "#         self.embed = 'fixed'\n",
        "#         self.freq = 'm'\n",
        "#         self.embed_type = 0\n",
        "#         self.activation = 'gelu'\n",
        "#         self.output_attention = False\n",
        "#         self.moving_avg = 25\n",
        "#         self.factor = 3\n",
        "#         self.distil = True  # ← add this line\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpTIs3Uhr8Qe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from math import sqrt\n",
        "import os\n",
        "\n",
        "\n",
        "class AutoCorrelation(nn.Module):\n",
        "    \"\"\"\n",
        "    AutoCorrelation Mechanism with the following two phases:\n",
        "    (1) period-based dependencies discovery\n",
        "    (2) time delay aggregation\n",
        "    This block can replace the self-attention family mechanism seamlessly.\n",
        "    \"\"\"\n",
        "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
        "        super(AutoCorrelation, self).__init__()\n",
        "        self.factor = factor\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def time_delay_agg_training(self, values, corr):\n",
        "        \"\"\"\n",
        "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
        "        This is for the training phase.\n",
        "        \"\"\"\n",
        "        head = values.shape[1]\n",
        "        channel = values.shape[2]\n",
        "        length = values.shape[3]\n",
        "        # find top k\n",
        "        top_k = int(self.factor * math.log(length))\n",
        "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
        "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
        "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
        "        # update corr\n",
        "        tmp_corr = torch.softmax(weights, dim=-1)\n",
        "        # aggregation\n",
        "        tmp_values = values\n",
        "        delays_agg = torch.zeros_like(values).float()\n",
        "        for i in range(top_k):\n",
        "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
        "            delays_agg = delays_agg + pattern * \\\n",
        "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
        "        return delays_agg\n",
        "\n",
        "    def time_delay_agg_inference(self, values, corr):\n",
        "        \"\"\"\n",
        "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
        "        This is for the inference phase.\n",
        "        \"\"\"\n",
        "        batch = values.shape[0]\n",
        "        head = values.shape[1]\n",
        "        channel = values.shape[2]\n",
        "        length = values.shape[3]\n",
        "        # index init\n",
        "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch, head, channel, 1).cuda()\n",
        "        # find top k\n",
        "        top_k = int(self.factor * math.log(length))\n",
        "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
        "        weights = torch.topk(mean_value, top_k, dim=-1)[0]\n",
        "        delay = torch.topk(mean_value, top_k, dim=-1)[1]\n",
        "        # update corr\n",
        "        tmp_corr = torch.softmax(weights, dim=-1)\n",
        "        # aggregation\n",
        "        tmp_values = values.repeat(1, 1, 1, 2)\n",
        "        delays_agg = torch.zeros_like(values).float()\n",
        "        for i in range(top_k):\n",
        "            tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n",
        "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
        "            delays_agg = delays_agg + pattern * \\\n",
        "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
        "        return delays_agg\n",
        "\n",
        "    def time_delay_agg_full(self, values, corr):\n",
        "        \"\"\"\n",
        "        Standard version of Autocorrelation\n",
        "        \"\"\"\n",
        "        batch = values.shape[0]\n",
        "        head = values.shape[1]\n",
        "        channel = values.shape[2]\n",
        "        length = values.shape[3]\n",
        "        # index init\n",
        "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch, head, channel, 1).cuda()\n",
        "        # find top k\n",
        "        top_k = int(self.factor * math.log(length))\n",
        "        weights = torch.topk(corr, top_k, dim=-1)[0]\n",
        "        delay = torch.topk(corr, top_k, dim=-1)[1]\n",
        "        # update corr\n",
        "        tmp_corr = torch.softmax(weights, dim=-1)\n",
        "        # aggregation\n",
        "        tmp_values = values.repeat(1, 1, 1, 2)\n",
        "        delays_agg = torch.zeros_like(values).float()\n",
        "        for i in range(top_k):\n",
        "            tmp_delay = init_index + delay[..., i].unsqueeze(-1)\n",
        "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
        "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
        "        return delays_agg\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        if L > S:\n",
        "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
        "            values = torch.cat([values, zeros], dim=1)\n",
        "            keys = torch.cat([keys, zeros], dim=1)\n",
        "        else:\n",
        "            values = values[:, :L, :, :]\n",
        "            keys = keys[:, :L, :, :]\n",
        "\n",
        "        # period-based dependencies\n",
        "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
        "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
        "        res = q_fft * torch.conj(k_fft)\n",
        "        corr = torch.fft.irfft(res, dim=-1)\n",
        "\n",
        "        # time delay agg\n",
        "        if self.training:\n",
        "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
        "        else:\n",
        "            return (V.contiguous(), None)\n",
        "\n",
        "\n",
        "class AutoCorrelationLayer(nn.Module):\n",
        "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
        "                 d_values=None):\n",
        "        super(AutoCorrelationLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "\n",
        "        self.inner_correlation = correlation\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "\n",
        "        out, attn = self.inner_correlation(\n",
        "            queries,\n",
        "            keys,\n",
        "            values,\n",
        "            attn_mask\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8WaD-3ns8Fu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class my_Layernorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Special designed layernorm for the seasonal part\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(my_Layernorm, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_hat = self.layernorm(x)\n",
        "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
        "        return x_hat - bias\n",
        "\n",
        "\n",
        "class moving_avg(nn.Module):\n",
        "    \"\"\"\n",
        "    Moving average block to highlight the trend of time series\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(moving_avg, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class series_decomp(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size):\n",
        "        super(series_decomp, self).__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoformer encoder layer with the progressive decomposition architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
        "        self.decomp1 = series_decomp(moving_avg)\n",
        "        self.decomp2 = series_decomp(moving_avg)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        new_x, attn = self.attention(\n",
        "            x, x, x,\n",
        "            attn_mask=attn_mask\n",
        "        )\n",
        "        x = x + self.dropout(new_x)\n",
        "        x, _ = self.decomp1(x)\n",
        "        y = x\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "        res, _ = self.decomp2(x + y)\n",
        "        return res, attn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoformer encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attns = []\n",
        "        if self.conv_layers is not None:\n",
        "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                x = conv_layer(x)\n",
        "                attns.append(attn)\n",
        "            x, attn = self.attn_layers[-1](x)\n",
        "            attns.append(attn)\n",
        "        else:\n",
        "            for attn_layer in self.attn_layers:\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                attns.append(attn)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x, attns\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoformer decoder layer with the progressive decomposition architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n",
        "                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.self_attention = self_attention\n",
        "        self.cross_attention = cross_attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
        "        self.decomp1 = series_decomp(moving_avg)\n",
        "        self.decomp2 = series_decomp(moving_avg)\n",
        "        self.decomp3 = series_decomp(moving_avg)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n",
        "                                    padding_mode='circular', bias=False)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
        "        x = x + self.dropout(self.self_attention(\n",
        "            x, x, x,\n",
        "            attn_mask=x_mask\n",
        "        )[0])\n",
        "        x, trend1 = self.decomp1(x)\n",
        "        x = x + self.dropout(self.cross_attention(\n",
        "            x, cross, cross,\n",
        "            attn_mask=cross_mask\n",
        "        )[0])\n",
        "        x, trend2 = self.decomp2(x)\n",
        "        y = x\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "        x, trend3 = self.decomp3(x + y)\n",
        "\n",
        "        residual_trend = trend1 + trend2 + trend3\n",
        "        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x, residual_trend\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoformer encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, norm_layer=None, projection=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.norm = norm_layer\n",
        "        self.projection = projection\n",
        "\n",
        "    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n",
        "        for layer in self.layers:\n",
        "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
        "            trend = trend + residual_trend\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        if self.projection is not None:\n",
        "            x = self.projection(x)\n",
        "        return x, trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkOMRP5ytChg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FixedEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(FixedEmbedding, self).__init__()\n",
        "\n",
        "        w = torch.zeros(c_in, d_model).float()\n",
        "        w.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        w[:, 0::2] = torch.sin(position * div_term)\n",
        "        w[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.emb = nn.Embedding(c_in, d_model)\n",
        "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x).detach()\n",
        "\n",
        "\n",
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        minute_size = 4\n",
        "        hour_size = 24\n",
        "        weekday_size = 7\n",
        "        day_size = 32\n",
        "        month_size = 13\n",
        "\n",
        "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
        "        if freq == 't':\n",
        "            self.minute_embed = Embed(minute_size, d_model)\n",
        "        self.hour_embed = Embed(hour_size, d_model)\n",
        "        self.weekday_embed = Embed(weekday_size, d_model)\n",
        "        self.day_embed = Embed(day_size, d_model)\n",
        "        self.month_embed = Embed(month_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "\n",
        "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
        "        hour_x = self.hour_embed(x[:, :, 3])\n",
        "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
        "        day_x = self.day_embed(x[:, :, 1])\n",
        "        month_x = self.month_embed(x[:, :, 0])\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
        "\n",
        "\n",
        "class TimeFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
        "        super(TimeFeatureEmbedding, self).__init__()\n",
        "\n",
        "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
        "        d_inp = freq_map[freq]\n",
        "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class DataEmbedding_wo_pos(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_pos, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class DataEmbedding_wo_pos_temp(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_pos_temp, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class DataEmbedding_wo_temp(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_temp, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah0_8PvAr19Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoformer is the first method to achieve the series-wise connection,\n",
        "    with inherent O(LlogL) complexity\n",
        "    \"\"\"\n",
        "    def __init__(self, configs):\n",
        "        super(Model, self).__init__()\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.label_len = configs.label_len\n",
        "        self.pred_len = configs.pred_len\n",
        "        self.output_attention = configs.output_attention\n",
        "\n",
        "        # Decomp\n",
        "        kernel_size = configs.moving_avg\n",
        "        self.decomp = series_decomp(kernel_size)\n",
        "\n",
        "        # Embedding\n",
        "        # The series-wise connection inherently contains the sequential information.\n",
        "        # Thus, we can discard the position embedding of transformers.\n",
        "        if configs.embed_type == 0:\n",
        "            self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_pos(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "        elif configs.embed_type == 1:\n",
        "            self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "        elif configs.embed_type == 2:\n",
        "            self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_pos(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "\n",
        "        elif configs.embed_type == 3:\n",
        "            self.enc_embedding = DataEmbedding_wo_temp(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_temp(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "        elif configs.embed_type == 4:\n",
        "            self.enc_embedding = DataEmbedding_wo_pos_temp(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_pos_temp(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AutoCorrelationLayer(\n",
        "                        AutoCorrelation(False, configs.factor, attention_dropout=configs.dropout,\n",
        "                                        output_attention=configs.output_attention),\n",
        "                        configs.d_model, configs.n_heads),\n",
        "                    configs.d_model,\n",
        "                    configs.d_ff,\n",
        "                    moving_avg=configs.moving_avg,\n",
        "                    dropout=configs.dropout,\n",
        "                    activation=configs.activation\n",
        "                ) for l in range(configs.e_layers)\n",
        "            ],\n",
        "            norm_layer=my_Layernorm(configs.d_model)\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(\n",
        "            [\n",
        "                DecoderLayer(\n",
        "                    AutoCorrelationLayer(\n",
        "                        AutoCorrelation(True, configs.factor, attention_dropout=configs.dropout,\n",
        "                                        output_attention=False),\n",
        "                        configs.d_model, configs.n_heads),\n",
        "                    AutoCorrelationLayer(\n",
        "                        AutoCorrelation(False, configs.factor, attention_dropout=configs.dropout,\n",
        "                                        output_attention=False),\n",
        "                        configs.d_model, configs.n_heads),\n",
        "                    configs.d_model,\n",
        "                    configs.c_out,\n",
        "                    configs.d_ff,\n",
        "                    moving_avg=configs.moving_avg,\n",
        "                    dropout=configs.dropout,\n",
        "                    activation=configs.activation,\n",
        "                )\n",
        "                for l in range(configs.d_layers)\n",
        "            ],\n",
        "            norm_layer=my_Layernorm(configs.d_model),\n",
        "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
        "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "        # decomp init\n",
        "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
        "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]], device=x_enc.device)\n",
        "        seasonal_init, trend_init = self.decomp(x_enc)\n",
        "        # decoder input\n",
        "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
        "        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)\n",
        "        # enc\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
        "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
        "        # dec\n",
        "        dec_out = self.dec_embedding(seasonal_init, x_mark_dec)\n",
        "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
        "                                                 trend=trend_init)\n",
        "        # final\n",
        "        dec_out = trend_part + seasonal_part\n",
        "\n",
        "        if self.output_attention:\n",
        "            return dec_out[:, -self.pred_len:, :], attns\n",
        "        else:\n",
        "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ihy_blP5H2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "configs = AutoformerConfigs()\n",
        "\n",
        "\n",
        "seq_len = 264\n",
        "pred_len = 0\n",
        "\n",
        "x = torch.tensor(X_grid[:, :seq_len], dtype=torch.float32)\n",
        "y = torch.tensor(y_grid[:, :seq_len], dtype=torch.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIECa7VAL1Yz"
      },
      "outputs": [],
      "source": [
        "class GridTimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx].unsqueeze(-1), self.y[idx].unsqueeze(-1)  # (256, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBVPiIxWL1Ov"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(GridTimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(GridTimeSeriesDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ikpeGhFMfy9"
      },
      "outputs": [],
      "source": [
        "pred_len = 264  # or whatever value you're using\n",
        "x_dec = torch.zeros_like(xb[:, -pred_len:, :])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPXPCiICP5GA"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Model(configs).to(device)  # This uses your PyTorch Autoformer class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmlHE7LPOYjc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(configs).to(device)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmYTa6RvM8WQ",
        "outputId": "ed6993a4-ccf5-486e-8c8f-a17e4ac0a58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 — Train Loss: 0.018962, Val Loss: 0.017620\n",
            "Epoch 2/2 — Train Loss: 0.008606, Val Loss: 0.014410\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # Ensure decoder input length = pred_len\n",
        "        x_dec = torch.zeros((xb.size(0), model.pred_len, 1), device=device)\n",
        "\n",
        "        # Time features — must match model's expectation\n",
        "        x_mark_enc = torch.zeros((xb.size(0), model.seq_len, 4), device=device)\n",
        "        x_mark_dec = torch.zeros((xb.size(0), model.label_len + model.pred_len, 4), device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(xb, x_mark_enc, x_dec, x_mark_dec)\n",
        "\n",
        "        if isinstance(output, tuple):\n",
        "            output = output[0]\n",
        "\n",
        "        loss = criterion(output, yb[:, -model.pred_len:, :])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            x_dec = torch.zeros((xb.size(0), model.pred_len, 1), device=device)\n",
        "            x_mark_enc = torch.zeros((xb.size(0), model.seq_len, 4), device=device)\n",
        "            x_mark_dec = torch.zeros((xb.size(0), model.label_len + model.pred_len, 4), device=device)\n",
        "\n",
        "            val_output = model(xb, x_mark_enc, x_dec, x_mark_dec)\n",
        "            if isinstance(val_output, tuple):\n",
        "                val_output = val_output[0]\n",
        "\n",
        "            loss = criterion(val_output, yb[:, -model.pred_len:, :])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gAifjSDzhK8"
      },
      "source": [
        "#Informer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opvLrpV_Ibr9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
        "                                  out_channels=c_in,\n",
        "                                  kernel_size=3,\n",
        "                                  padding=2,\n",
        "                                  padding_mode='circular')\n",
        "        self.norm = nn.BatchNorm1d(c_in)\n",
        "        self.activation = nn.ELU()\n",
        "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downConv(x.permute(0, 2, 1))\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.maxPool(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        new_x, attn = self.attention(\n",
        "            x, x, x,\n",
        "            attn_mask=attn_mask\n",
        "        )\n",
        "        x = x + self.dropout(new_x)\n",
        "\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "\n",
        "        return self.norm2(x + y), attn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # x [B, L, D]\n",
        "        attns = []\n",
        "        if self.conv_layers is not None:\n",
        "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                x = conv_layer(x)\n",
        "                attns.append(attn)\n",
        "            x, attn = self.attn_layers[-1](x)\n",
        "            attns.append(attn)\n",
        "        else:\n",
        "            for attn_layer in self.attn_layers:\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                attns.append(attn)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x, attns\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
        "                 dropout=0.1, activation=\"relu\"):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.self_attention = self_attention\n",
        "        self.cross_attention = cross_attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
        "        x = x + self.dropout(self.self_attention(\n",
        "            x, x, x,\n",
        "            attn_mask=x_mask\n",
        "        )[0])\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = x + self.dropout(self.cross_attention(\n",
        "            x, cross, cross,\n",
        "            attn_mask=cross_mask\n",
        "        )[0])\n",
        "\n",
        "        y = x = self.norm2(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "\n",
        "        return self.norm3(x + y)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers, norm_layer=None, projection=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.norm = norm_layer\n",
        "        self.projection = projection\n",
        "\n",
        "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        if self.projection is not None:\n",
        "            x = self.projection(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9o3-qOLIuzC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class TriangularCausalMask():\n",
        "    def __init__(self, B, L, device=\"cpu\"):\n",
        "        mask_shape = [B, 1, L, L]\n",
        "        with torch.no_grad():\n",
        "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask\n",
        "\n",
        "\n",
        "class ProbMask():\n",
        "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
        "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
        "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
        "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
        "                    torch.arange(H)[None, :, None],\n",
        "                    index, :].to(device)\n",
        "        self._mask = indicator.view(scores.shape).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QixBGS2vIboS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from math import sqrt\n",
        "import os\n",
        "\n",
        "\n",
        "class FullAttention(nn.Module):\n",
        "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
        "        super(FullAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        scale = self.scale or 1. / sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
        "\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "\n",
        "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return (V.contiguous(), A)\n",
        "        else:\n",
        "            return (V.contiguous(), None)\n",
        "\n",
        "\n",
        "class ProbAttention(nn.Module):\n",
        "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
        "        super(ProbAttention, self).__init__()\n",
        "        self.factor = factor\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n",
        "        # Q [B, H, L, D]\n",
        "        B, H, L_K, E = K.shape\n",
        "        _, _, L_Q, _ = Q.shape\n",
        "\n",
        "        # calculate the sampled Q_K\n",
        "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
        "        index_sample = torch.randint(L_K, (L_Q, sample_k))  # real U = U_part(factor*ln(L_k))*L_q\n",
        "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
        "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
        "\n",
        "        # find the Top_k query with sparisty measurement\n",
        "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
        "        M_top = M.topk(n_top, sorted=False)[1]\n",
        "\n",
        "        # use the reduced Q to calculate Q_K\n",
        "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
        "                   torch.arange(H)[None, :, None],\n",
        "                   M_top, :]  # factor*ln(L_q)\n",
        "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # factor*ln(L_q)*L_k\n",
        "\n",
        "        return Q_K, M_top\n",
        "\n",
        "    def _get_initial_context(self, V, L_Q):\n",
        "        B, H, L_V, D = V.shape\n",
        "        if not self.mask_flag:\n",
        "            # V_sum = V.sum(dim=-2)\n",
        "            V_sum = V.mean(dim=-2)\n",
        "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
        "        else:  # use mask\n",
        "            assert (L_Q == L_V)  # requires that L_Q == L_V, i.e. for self-attention only\n",
        "            contex = V.cumsum(dim=-2)\n",
        "        return contex\n",
        "\n",
        "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
        "        B, H, L_V, D = V.shape\n",
        "\n",
        "        if self.mask_flag:\n",
        "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)  # nn.Softmax(dim=-1)(scores)\n",
        "\n",
        "        context_in[torch.arange(B)[:, None, None],\n",
        "        torch.arange(H)[None, :, None],\n",
        "        index, :] = torch.matmul(attn, V).type_as(context_in)\n",
        "        if self.output_attention:\n",
        "            attns = (torch.ones([B, H, L_V, L_V]) / L_V).type_as(attn).to(attn.device)\n",
        "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
        "            return (context_in, attns)\n",
        "        else:\n",
        "            return (context_in, None)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L_Q, H, D = queries.shape\n",
        "        _, L_K, _, _ = keys.shape\n",
        "\n",
        "        queries = queries.transpose(2, 1)\n",
        "        keys = keys.transpose(2, 1)\n",
        "        values = values.transpose(2, 1)\n",
        "\n",
        "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()  # c*ln(L_k)\n",
        "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()  # c*ln(L_q)\n",
        "\n",
        "        U_part = U_part if U_part < L_K else L_K\n",
        "        u = u if u < L_Q else L_Q\n",
        "\n",
        "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
        "\n",
        "        # add scale factor\n",
        "        scale = self.scale or 1. / sqrt(D)\n",
        "        if scale is not None:\n",
        "            scores_top = scores_top * scale\n",
        "        # get the context\n",
        "        context = self._get_initial_context(values, L_Q)\n",
        "        # update the context with selected top_k queries\n",
        "        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
        "\n",
        "        return context.contiguous(), attn\n",
        "\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
        "                 d_values=None):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "\n",
        "        out, attn = self.inner_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values,\n",
        "            attn_mask\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTaIBsU_Iblo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FixedEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(FixedEmbedding, self).__init__()\n",
        "\n",
        "        w = torch.zeros(c_in, d_model).float()\n",
        "        w.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        w[:, 0::2] = torch.sin(position * div_term)\n",
        "        w[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.emb = nn.Embedding(c_in, d_model)\n",
        "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x).detach()\n",
        "\n",
        "\n",
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        minute_size = 4\n",
        "        hour_size = 24\n",
        "        weekday_size = 7\n",
        "        day_size = 32\n",
        "        month_size = 13\n",
        "\n",
        "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
        "        if freq == 't':\n",
        "            self.minute_embed = Embed(minute_size, d_model)\n",
        "        self.hour_embed = Embed(hour_size, d_model)\n",
        "        self.weekday_embed = Embed(weekday_size, d_model)\n",
        "        self.day_embed = Embed(day_size, d_model)\n",
        "        self.month_embed = Embed(month_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "\n",
        "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
        "        hour_x = self.hour_embed(x[:, :, 3])\n",
        "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
        "        day_x = self.day_embed(x[:, :, 1])\n",
        "        month_x = self.month_embed(x[:, :, 0])\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
        "\n",
        "\n",
        "class TimeFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
        "        super(TimeFeatureEmbedding, self).__init__()\n",
        "\n",
        "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
        "        d_inp = freq_map[freq]\n",
        "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class DataEmbedding_wo_pos(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_pos, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class DataEmbedding_wo_pos_temp(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_pos_temp, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class DataEmbedding_wo_temp(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_temp, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2rb77DZIbdM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5FK0Bh2Ll8H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Informer_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Informer with Propspare attention in O(LlogL) complexity\n",
        "    \"\"\"\n",
        "    def __init__(self, configs):\n",
        "        super(Informer_Model, self).__init__()\n",
        "        self.pred_len = configs.pred_len\n",
        "        configs.pred_len = configs.seq_len  # same length output as input\n",
        "        configs.label_len = configs.seq_len\n",
        "\n",
        "        self.output_attention = configs.output_attention\n",
        "\n",
        "        # Embedding\n",
        "        if configs.embed_type == 0:\n",
        "            self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                            configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                           configs.dropout)\n",
        "        elif configs.embed_type == 1:\n",
        "            self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "        elif configs.embed_type == 2:\n",
        "            self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_pos(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "\n",
        "        elif configs.embed_type == 3:\n",
        "            self.enc_embedding = DataEmbedding_wo_temp(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_temp(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "        elif configs.embed_type == 4:\n",
        "            self.enc_embedding = DataEmbedding_wo_pos_temp(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "            self.dec_embedding = DataEmbedding_wo_pos_temp(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                                    configs.dropout)\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(\n",
        "                        ProbAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
        "                                      output_attention=configs.output_attention),\n",
        "                        configs.d_model, configs.n_heads),\n",
        "                    configs.d_model,\n",
        "                    configs.d_ff,\n",
        "                    dropout=configs.dropout,\n",
        "                    activation=configs.activation\n",
        "                ) for l in range(configs.e_layers)\n",
        "            ],\n",
        "            [\n",
        "                ConvLayer(\n",
        "                    configs.d_model\n",
        "                ) for l in range(configs.e_layers - 1)\n",
        "            ] if configs.distil else None,\n",
        "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(\n",
        "            [\n",
        "                DecoderLayer(\n",
        "                    AttentionLayer(\n",
        "                        ProbAttention(True, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
        "                        configs.d_model, configs.n_heads),\n",
        "                    AttentionLayer(\n",
        "                        ProbAttention(False, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
        "                        configs.d_model, configs.n_heads),\n",
        "                    configs.d_model,\n",
        "                    configs.d_ff,\n",
        "                    dropout=configs.dropout,\n",
        "                    activation=configs.activation,\n",
        "                )\n",
        "                for l in range(configs.d_layers)\n",
        "            ],\n",
        "            norm_layer=torch.nn.LayerNorm(configs.d_model),\n",
        "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
        "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
        "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
        "\n",
        "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
        "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return dec_out, attns\n",
        "        else:\n",
        "            return dec_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjbW9_u-Qy2Z"
      },
      "outputs": [],
      "source": [
        "class InformerConfigs:\n",
        "    def __init__(self):\n",
        "        self.seq_len = 264\n",
        "        self.label_len = 264\n",
        "        self.pred_len = 264\n",
        "        self.enc_in = 1\n",
        "        self.dec_in = 1\n",
        "        self.c_out = 1\n",
        "        self.d_model = 64\n",
        "        self.n_heads = 4\n",
        "        self.d_ff = 128\n",
        "        self.e_layers = 2\n",
        "        self.d_layers = 1\n",
        "        self.dropout = 0.1\n",
        "        self.embed = 'fixed'\n",
        "        self.freq = 'm'\n",
        "        self.embed_type = 0\n",
        "        self.activation = 'gelu'\n",
        "        self.output_attention = False\n",
        "        self.factor = 3\n",
        "        self.distil = True  # ✅ this is required by Informer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctGhvjioRCO8"
      },
      "outputs": [],
      "source": [
        "configs = InformerConfigs()\n",
        "model = Informer_Model(configs).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTEucNATQEfT",
        "outputId": "35ddd15c-a999-4967-d6f5-0b2dff1c175a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tqz8eu_PHr3",
        "outputId": "5374f475-5fc0-4956-f610-2520967096a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 — Train Loss: 0.014973, Val Loss: 0.007972\n",
            "Epoch 2/2 — Train Loss: 0.007775, Val Loss: 0.009270\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------- Dataset --------\n",
        "class GridTimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.Y = torch.from_numpy(Y).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].unsqueeze(-1)  # [seq_len, 1]\n",
        "        y = self.Y[idx].unsqueeze(-1)\n",
        "        return x, y\n",
        "\n",
        "# -------- Data --------\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(GridTimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(GridTimeSeriesDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# -------- Model --------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Informer_Model(configs).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 2\n",
        "\n",
        "# -------- Training Loop --------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x_enc, y in train_loader:\n",
        "        x_enc, y = x_enc.to(device), y.to(device)\n",
        "\n",
        "        # Dummy time features (you can replace with real ones if needed)\n",
        "        x_mark_enc = torch.zeros((x_enc.size(0), x_enc.size(1), 4), device=device)\n",
        "\n",
        "        # Input = Output = same seq_len\n",
        "        x_dec = x_enc.clone()\n",
        "        x_mark_dec = x_mark_enc.clone()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
        "\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # -------- Validation --------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x_enc, y in val_loader:\n",
        "            x_enc, y = x_enc.to(device), y.to(device)\n",
        "            x_mark_enc = torch.zeros((x_enc.size(0), x_enc.size(1), 4), device=device)\n",
        "            x_dec = x_enc.clone()\n",
        "            x_mark_dec = x_mark_enc.clone()\n",
        "\n",
        "            output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
        "            loss = criterion(output, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30jfd4bShGrP"
      },
      "source": [
        "# TimesFM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timesfm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLWhXtf2b5ra",
        "outputId": "9e3f32ae-8f37-4676-cf26-3c1a995e4662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timesfm in /usr/local/lib/python3.11/dist-packages (1.2.9)\n",
            "Requirement already satisfied: absl-py>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from timesfm) (1.4.0)\n",
            "Requirement already satisfied: einshape>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from timesfm) (1.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]>=0.23.0->timesfm) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from timesfm) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from timesfm) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from timesfm) (1.6.1)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from timesfm) (0.16.0)\n",
            "Requirement already satisfied: utilsforecast>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from timesfm) (0.2.12)\n",
            "Requirement already satisfied: wandb>=0.17.5 in /usr/local/lib/python3.11/dist-packages (from timesfm) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (1.1.4)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]>=0.23.0->timesfm) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm) (3.0.51)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->timesfm) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->timesfm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->timesfm) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->timesfm) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->timesfm) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->timesfm) (3.6.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->timesfm) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->timesfm) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->timesfm) (13.9.4)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (2.30.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm) (1.3.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.5->timesfm) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.17.5->timesfm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.17.5->timesfm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.17.5->timesfm) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->timesfm) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm) (2025.6.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.12.3->timesfm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.12.3->timesfm) (2.19.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.5->timesfm) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->timesfm) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GridTimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X_series, y_series):\n",
        "        \"\"\"\n",
        "        X_series, y_series: shape (num_series, time_steps)\n",
        "        \"\"\"\n",
        "        self.X = X_series\n",
        "        self.y = y_series\n",
        "        self.freq = torch.tensor([0])  # Use 0 unless you're doing frequency-aware training\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_context = torch.tensor(self.X[idx], dtype=torch.float32)   # (T,)\n",
        "        x_future = torch.tensor(self.y[idx], dtype=torch.float32)    # (T,)\n",
        "        x_padding = torch.zeros_like(x_context)\n",
        "        return x_context, x_padding, self.freq, x_future\n"
      ],
      "metadata": {
        "id": "JyV5PzZ-b5pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TimesFM Finetuner: A flexible framework for finetuning TimesFM models on custom datasets.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "class MetricsLogger(ABC):\n",
        "  \"\"\"Abstract base class for logging metrics during training.\n",
        "\n",
        "    This class defines the interface for logging metrics during model training.\n",
        "    Concrete implementations can log to different backends (e.g., WandB, TensorBoard).\n",
        "    \"\"\"\n",
        "\n",
        "  @abstractmethod\n",
        "  def log_metrics(self,\n",
        "                  metrics: Dict[str, Any],\n",
        "                  step: Optional[int] = None) -> None:\n",
        "    \"\"\"Log metrics to the specified backend.\n",
        "\n",
        "        Args:\n",
        "          metrics: Dictionary containing metric names and values.\n",
        "          step: Optional step number or epoch for the metrics.\n",
        "        \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def close(self) -> None:\n",
        "    \"\"\"Clean up any resources used by the logger.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class WandBLogger(MetricsLogger):\n",
        "  \"\"\"Weights & Biases implementation of metrics logging.\n",
        "\n",
        "    Args:\n",
        "      project: Name of the W&B project.\n",
        "      config: Configuration dictionary to log.\n",
        "      rank: Process rank in distributed training.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self, project: str, config: Dict[str, Any], rank: int = 0):\n",
        "    self.rank = rank\n",
        "    if rank == 0:\n",
        "      wandb.init(project=project, config=config)\n",
        "\n",
        "  def log_metrics(self,\n",
        "                  metrics: Dict[str, Any],\n",
        "                  step: Optional[int] = None) -> None:\n",
        "    \"\"\"Log metrics to W&B if on the main process.\n",
        "\n",
        "        Args:\n",
        "          metrics: Dictionary of metrics to log.\n",
        "          step: Current training step or epoch.\n",
        "        \"\"\"\n",
        "    if self.rank == 0:\n",
        "      wandb.log(metrics, step=step)\n",
        "\n",
        "  def close(self) -> None:\n",
        "    \"\"\"Finish the W&B run if on the main process.\"\"\"\n",
        "    if self.rank == 0:\n",
        "      wandb.finish()\n",
        "\n",
        "\n",
        "class DistributedManager:\n",
        "  \"\"\"Manages distributed training setup and cleanup.\n",
        "\n",
        "    Args:\n",
        "      world_size: Total number of processes.\n",
        "      rank: Process rank.\n",
        "      master_addr: Address of the master process.\n",
        "      master_port: Port for distributed communication.\n",
        "      backend: PyTorch distributed backend to use.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      world_size: int,\n",
        "      rank: int,\n",
        "      master_addr: str = \"localhost\",\n",
        "      master_port: str = \"12358\",\n",
        "      backend: str = \"nccl\",\n",
        "  ):\n",
        "    self.world_size = world_size\n",
        "    self.rank = rank\n",
        "    self.master_addr = master_addr\n",
        "    self.master_port = master_port\n",
        "    self.backend = backend\n",
        "\n",
        "  def setup(self) -> None:\n",
        "    \"\"\"Initialize the distributed environment.\"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = self.master_addr\n",
        "    os.environ[\"MASTER_PORT\"] = self.master_port\n",
        "\n",
        "    if not dist.is_initialized():\n",
        "      dist.init_process_group(backend=self.backend,\n",
        "                              world_size=self.world_size,\n",
        "                              rank=self.rank)\n",
        "\n",
        "  def cleanup(self) -> None:\n",
        "    \"\"\"Clean up the distributed environment.\"\"\"\n",
        "    if dist.is_initialized():\n",
        "      dist.destroy_process_group()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FinetuningConfig:\n",
        "  \"\"\"Configuration for model training.\n",
        "\n",
        "    Args:\n",
        "      batch_size: Number of samples per batch.\n",
        "      num_epochs: Number of training epochs.\n",
        "      learning_rate: Initial learning rate.\n",
        "      weight_decay: L2 regularization factor.\n",
        "      freq_type: Frequency, can be [0, 1, 2].\n",
        "      use_quantile_loss: bool = False  # Flag to enable/disable quantile loss\n",
        "      quantiles: Optional[List[float]] = None\n",
        "      device: Device to train on ('cuda' or 'cpu').\n",
        "      distributed: Whether to use distributed training.\n",
        "      gpu_ids: List of GPU IDs to use.\n",
        "      master_port: Port for distributed training.\n",
        "      master_addr: Address for distributed training.\n",
        "      use_wandb: Whether to use Weights & Biases logging.\n",
        "      wandb_project: W&B project name.\n",
        "      log_every_n_steps: Log metrics every N steps (batches), this is inspired from Pytorch Lightning\n",
        "      val_check_interval: How often within one training epoch to check val metrics. (also from Pytorch Lightning)\n",
        "        Can be: float (0.0-1.0): fraction of epoch (e.g., 0.5 = validate twice per epoch)\n",
        "                int: validate every N batches\n",
        "    \"\"\"\n",
        "\n",
        "  batch_size: int = 32\n",
        "  num_epochs: int = 20\n",
        "  learning_rate: float = 1e-4\n",
        "  weight_decay: float = 0.01\n",
        "  freq_type: int = 0\n",
        "  use_quantile_loss: bool = False\n",
        "  quantiles: Optional[List[float]] = None\n",
        "  device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  distributed: bool = False\n",
        "  gpu_ids: List[int] = field(default_factory=lambda: [0])\n",
        "  master_port: str = \"12358\"\n",
        "  master_addr: str = \"localhost\"\n",
        "  use_wandb: bool = False\n",
        "  wandb_project: str = \"timesfm-finetuning\"\n",
        "  log_every_n_steps: int = 50\n",
        "  val_check_interval: float = 0.5\n",
        "\n",
        "\n",
        "class TimesFMFinetuner:\n",
        "  \"\"\"Handles model training and validation.\n",
        "\n",
        "    Args:\n",
        "      model: PyTorch model to train.\n",
        "      config: Training configuration.\n",
        "      rank: Process rank for distributed training.\n",
        "      loss_fn: Loss function (defaults to MSE).\n",
        "      logger: Optional logging.Logger instance.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model: nn.Module,\n",
        "      config: FinetuningConfig,\n",
        "      rank: int = 0,\n",
        "      loss_fn: Optional[Callable] = None,\n",
        "      logger: Optional[logging.Logger] = None,\n",
        "  ):\n",
        "    self.model = model\n",
        "    self.config = config\n",
        "    self.rank = rank\n",
        "    self.logger = logger or logging.getLogger(__name__)\n",
        "    self.device = torch.device(\n",
        "        f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.loss_fn = loss_fn or (lambda x, y: torch.mean((x - y.squeeze(-1))**2))\n",
        "\n",
        "    if config.use_wandb:\n",
        "      self.metrics_logger = WandBLogger(config.wandb_project, config.__dict__,\n",
        "                                        rank)\n",
        "\n",
        "    if config.distributed:\n",
        "      self.dist_manager = DistributedManager(\n",
        "          world_size=len(config.gpu_ids),\n",
        "          rank=rank,\n",
        "          master_addr=config.master_addr,\n",
        "          master_port=config.master_port,\n",
        "      )\n",
        "      self.dist_manager.setup()\n",
        "      self.model = self._setup_distributed_model()\n",
        "\n",
        "  def _setup_distributed_model(self) -> nn.Module:\n",
        "    \"\"\"Configure model for distributed training.\"\"\"\n",
        "    self.model = self.model.to(self.device)\n",
        "    return DDP(self.model,\n",
        "               device_ids=[self.config.gpu_ids[self.rank]],\n",
        "               output_device=self.config.gpu_ids[self.rank])\n",
        "\n",
        "  def _create_dataloader(self, dataset: Dataset, is_train: bool) -> DataLoader:\n",
        "    \"\"\"Create appropriate DataLoader based on training configuration.\n",
        "\n",
        "        Args:\n",
        "          dataset: Dataset to create loader for.\n",
        "          is_train: Whether this is for training (affects shuffling).\n",
        "\n",
        "        Returns:\n",
        "          DataLoader instance.\n",
        "        \"\"\"\n",
        "    if self.config.distributed:\n",
        "      sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          dataset,\n",
        "          num_replicas=len(self.config.gpu_ids),\n",
        "          rank=dist.get_rank(),\n",
        "          shuffle=is_train)\n",
        "    else:\n",
        "      sampler = None\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=self.config.batch_size,\n",
        "        shuffle=(is_train and not self.config.distributed),\n",
        "        sampler=sampler,\n",
        "    )\n",
        "\n",
        "  def _quantile_loss(self, pred: torch.Tensor, actual: torch.Tensor,\n",
        "                     quantile: float) -> torch.Tensor:\n",
        "    \"\"\"Calculates quantile loss.\n",
        "        Args:\n",
        "            pred: Predicted values\n",
        "            actual: Actual values\n",
        "            quantile: Quantile at which loss is computed\n",
        "        Returns:\n",
        "            Quantile loss\n",
        "        \"\"\"\n",
        "    dev = actual - pred\n",
        "    loss_first = dev * quantile\n",
        "    loss_second = -dev * (1.0 - quantile)\n",
        "    return 2 * torch.where(loss_first >= 0, loss_first, loss_second)\n",
        "\n",
        "  def _process_batch(self, batch: List[torch.Tensor]) -> tuple:\n",
        "    \"\"\"Process a single batch of data.\n",
        "\n",
        "        Args:\n",
        "          batch: List of input tensors.\n",
        "\n",
        "        Returns:\n",
        "          Tuple of (loss, predictions).\n",
        "        \"\"\"\n",
        "    x_context, x_padding, freq, x_future = [\n",
        "        t.to(self.device, non_blocking=True) for t in batch\n",
        "    ]\n",
        "\n",
        "    predictions = self.model(x_context, x_padding.float(), freq)\n",
        "    predictions_mean = predictions[..., 0]\n",
        "    last_patch_pred = predictions_mean[:, -1, :]\n",
        "\n",
        "    loss = self.loss_fn(last_patch_pred, x_future.squeeze(-1))\n",
        "    if self.config.use_quantile_loss:\n",
        "      quantiles = self.config.quantiles or create_quantiles()\n",
        "      for i, quantile in enumerate(quantiles):\n",
        "        last_patch_quantile = predictions[:, -1, :, i + 1]\n",
        "        loss += torch.mean(\n",
        "            self._quantile_loss(last_patch_quantile, x_future.squeeze(-1),\n",
        "                                quantile))\n",
        "\n",
        "    return loss, predictions\n",
        "\n",
        "  def _train_epoch(self, train_loader: DataLoader,\n",
        "                   optimizer: torch.optim.Optimizer) -> float:\n",
        "    \"\"\"Train for one epoch in a distributed setting.\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for training data.\n",
        "            optimizer: Optimizer instance.\n",
        "\n",
        "        Returns:\n",
        "            Average training loss for the epoch.\n",
        "        \"\"\"\n",
        "    self.model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for batch in train_loader:\n",
        "      loss, _ = self._process_batch(batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    if self.config.distributed:\n",
        "      avg_loss_tensor = torch.tensor(avg_loss, device=self.device)\n",
        "      dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)\n",
        "      avg_loss = (avg_loss_tensor / dist.get_world_size()).item()\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "  def _validate(self, val_loader: DataLoader) -> float:\n",
        "    \"\"\"Perform validation.\n",
        "\n",
        "        Args:\n",
        "            val_loader: DataLoader for validation data.\n",
        "\n",
        "        Returns:\n",
        "            Average validation loss.\n",
        "        \"\"\"\n",
        "    self.model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in val_loader:\n",
        "        loss, _ = self._process_batch(batch)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    if self.config.distributed:\n",
        "      avg_loss_tensor = torch.tensor(avg_loss, device=self.device)\n",
        "      dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)\n",
        "      avg_loss = (avg_loss_tensor / dist.get_world_size()).item()\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "  def finetune(self, train_dataset: Dataset,\n",
        "               val_dataset: Dataset) -> Dict[str, Any]:\n",
        "    \"\"\"Train the model.\n",
        "\n",
        "        Args:\n",
        "          train_dataset: Training dataset.\n",
        "          val_dataset: Validation dataset.\n",
        "\n",
        "        Returns:\n",
        "          Dictionary containing training history.\n",
        "        \"\"\"\n",
        "    self.model = self.model.to(self.device)\n",
        "    train_loader = self._create_dataloader(train_dataset, is_train=True)\n",
        "    val_loader = self._create_dataloader(val_dataset, is_train=False)\n",
        "\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                 lr=self.config.learning_rate,\n",
        "                                 weight_decay=self.config.weight_decay)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"learning_rate\": []}\n",
        "\n",
        "    self.logger.info(\n",
        "        f\"Starting training for {self.config.num_epochs} epochs...\")\n",
        "    self.logger.info(f\"Training samples: {len(train_dataset)}\")\n",
        "    self.logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    try:\n",
        "      for epoch in range(self.config.num_epochs):\n",
        "        train_loss = self._train_epoch(train_loader, optimizer)\n",
        "        val_loss = self._validate(val_loader)\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        metrics = {\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"learning_rate\": current_lr,\n",
        "            \"epoch\": epoch + 1,\n",
        "        }\n",
        "\n",
        "        if self.config.use_wandb:\n",
        "          self.metrics_logger.log_metrics(metrics)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"learning_rate\"].append(current_lr)\n",
        "\n",
        "        if self.rank == 0:\n",
        "          self.logger.info(\n",
        "              f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n",
        "          )\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "      self.logger.info(\"Training interrupted by user\")\n",
        "\n",
        "    if self.config.distributed:\n",
        "      self.dist_manager.cleanup()\n",
        "\n",
        "    if self.config.use_wandb:\n",
        "      self.metrics_logger.close()\n",
        "\n",
        "    return {\"history\": history}\n"
      ],
      "metadata": {
        "id": "9AqPaMMOeSbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from timesfm import TimesFm, TimesFmCheckpoint, TimesFmHparams\n",
        "from huggingface_hub import snapshot_download\n",
        "from os import path\n",
        "\n",
        "\n",
        "class TimesFmWrapper(nn.Module):\n",
        "    def __init__(self, context_len: int, horizon_len: int):\n",
        "        super().__init__()\n",
        "\n",
        "        repo_id = \"google/timesfm-2.0-500m-pytorch\"\n",
        "\n",
        "        # Model hyperparameters\n",
        "        hparams = TimesFmHparams(\n",
        "            backend=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            per_core_batch_size=32,\n",
        "            horizon_len=horizon_len,\n",
        "            context_len=context_len,\n",
        "            num_layers=50,\n",
        "            use_positional_embedding=False,\n",
        "        )\n",
        "\n",
        "        # Initialize TimesFmTorch model\n",
        "        tfm = TimesFm(hparams=hparams,\n",
        "                      checkpoint=TimesFmCheckpoint(huggingface_repo_id=repo_id))\n",
        "\n",
        "        # ✅ Unwrap actual PyTorch model inside TimesFmTorch\n",
        "        self.model = tfm._model\n",
        "\n",
        "        # ✅ Make parameters trainable\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x_context, x_padding, freq):\n",
        "        return self.model(x_context, x_padding, freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvenhvf9DySq",
        "outputId": "3f75f484-a1c9-4a3d-8819-a5a76f489393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
            "Loaded PyTorch TimesFM, likely because python version is 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your inputs\n",
        "context_len = 264\n",
        "horizon_len = 264\n",
        "\n",
        "# Instantiate model\n",
        "model = TimesFmWrapper(context_len, horizon_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "b231c95e262f491da36bc757fbc2532a",
            "fd7d2988ae214405aac81065a4a16b02",
            "2cac7065229b4cd696bc8e8dfa19203c",
            "120bf5a215544418b27865ba6bd7695a",
            "10c7a0c83f7a47b281f98ac22bca25e4",
            "e363c282fbc945a6b793aa2f4373af07",
            "1b34ffa2fe524a649ab9b1d09adc76ea",
            "1795320317a742b5a50f8ad6d411682e",
            "cfb98964dea1486aad12427f66075df0",
            "7a1ca5dd5f65435e9cacc2419f0bd0f4",
            "8908bcc711c84c5d956225442af81fa9"
          ]
        },
        "id": "qaFlwCuhD8cD",
        "outputId": "2502ad3a-4548-4c66-e7bb-d7290dfde9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b231c95e262f491da36bc757fbc2532a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"✅ Trainable parameters: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QwlACr2ECSW",
        "outputId": "ed1bcdcc-13f8-4b7c-b4bc-f2dcf2b7c72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Trainable parameters: 498828960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape, param.requires_grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wy9WFsODETq",
        "outputId": "c0c5acfb-79d1-4513-d6c9-bf264db4e93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.input_ff_layer.hidden_layer.0.weight torch.Size([1280, 64]) True\n",
            "model.input_ff_layer.hidden_layer.0.bias torch.Size([1280]) True\n",
            "model.input_ff_layer.output_layer.weight torch.Size([1280, 1280]) True\n",
            "model.input_ff_layer.output_layer.bias torch.Size([1280]) True\n",
            "model.input_ff_layer.residual_layer.weight torch.Size([1280, 64]) True\n",
            "model.input_ff_layer.residual_layer.bias torch.Size([1280]) True\n",
            "model.freq_emb.weight torch.Size([3, 1280]) True\n",
            "model.horizon_ff_layer.hidden_layer.0.weight torch.Size([1280, 1280]) True\n",
            "model.horizon_ff_layer.hidden_layer.0.bias torch.Size([1280]) True\n",
            "model.horizon_ff_layer.output_layer.weight torch.Size([1280, 1280]) True\n",
            "model.horizon_ff_layer.output_layer.bias torch.Size([1280]) True\n",
            "model.horizon_ff_layer.residual_layer.weight torch.Size([1280, 1280]) True\n",
            "model.horizon_ff_layer.residual_layer.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.0.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.0.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.0.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.0.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.0.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.0.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.0.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.0.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.0.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.0.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.0.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.0.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.1.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.1.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.1.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.1.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.1.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.1.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.1.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.1.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.1.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.1.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.1.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.1.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.2.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.2.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.2.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.2.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.2.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.2.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.2.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.2.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.2.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.2.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.2.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.2.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.3.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.3.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.3.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.3.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.3.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.3.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.3.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.3.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.3.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.3.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.3.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.3.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.4.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.4.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.4.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.4.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.4.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.4.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.4.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.4.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.4.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.4.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.4.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.4.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.5.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.5.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.5.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.5.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.5.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.5.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.5.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.5.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.5.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.5.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.5.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.5.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.6.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.6.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.6.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.6.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.6.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.6.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.6.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.6.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.6.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.6.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.6.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.6.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.7.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.7.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.7.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.7.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.7.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.7.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.7.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.7.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.7.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.7.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.7.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.7.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.8.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.8.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.8.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.8.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.8.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.8.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.8.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.8.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.8.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.8.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.8.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.8.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.9.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.9.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.9.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.9.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.9.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.9.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.9.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.9.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.9.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.9.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.9.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.9.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.10.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.10.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.10.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.10.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.10.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.10.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.10.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.10.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.10.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.10.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.10.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.10.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.11.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.11.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.11.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.11.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.11.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.11.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.11.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.11.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.11.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.11.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.11.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.11.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.12.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.12.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.12.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.12.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.12.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.12.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.12.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.12.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.12.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.12.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.12.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.12.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.13.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.13.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.13.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.13.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.13.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.13.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.13.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.13.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.13.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.13.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.13.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.13.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.14.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.14.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.14.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.14.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.14.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.14.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.14.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.14.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.14.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.14.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.14.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.14.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.15.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.15.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.15.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.15.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.15.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.15.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.15.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.15.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.15.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.15.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.15.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.15.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.16.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.16.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.16.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.16.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.16.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.16.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.16.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.16.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.16.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.16.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.16.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.16.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.17.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.17.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.17.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.17.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.17.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.17.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.17.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.17.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.17.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.17.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.17.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.17.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.18.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.18.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.18.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.18.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.18.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.18.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.18.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.18.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.18.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.18.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.18.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.18.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.19.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.19.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.19.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.19.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.19.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.19.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.19.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.19.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.19.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.19.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.19.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.19.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.20.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.20.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.20.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.20.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.20.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.20.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.20.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.20.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.20.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.20.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.20.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.20.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.21.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.21.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.21.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.21.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.21.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.21.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.21.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.21.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.21.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.21.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.21.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.21.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.22.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.22.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.22.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.22.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.22.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.22.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.22.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.22.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.22.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.22.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.22.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.22.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.23.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.23.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.23.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.23.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.23.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.23.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.23.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.23.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.23.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.23.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.23.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.23.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.24.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.24.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.24.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.24.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.24.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.24.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.24.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.24.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.24.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.24.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.24.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.24.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.25.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.25.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.25.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.25.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.25.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.25.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.25.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.25.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.25.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.25.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.25.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.25.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.26.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.26.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.26.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.26.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.26.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.26.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.26.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.26.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.26.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.26.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.26.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.26.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.27.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.27.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.27.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.27.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.27.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.27.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.27.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.27.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.27.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.27.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.27.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.27.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.28.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.28.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.28.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.28.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.28.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.28.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.28.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.28.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.28.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.28.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.28.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.28.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.29.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.29.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.29.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.29.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.29.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.29.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.29.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.29.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.29.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.29.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.29.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.29.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.30.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.30.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.30.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.30.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.30.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.30.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.30.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.30.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.30.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.30.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.30.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.30.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.31.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.31.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.31.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.31.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.31.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.31.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.31.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.31.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.31.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.31.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.31.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.31.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.32.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.32.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.32.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.32.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.32.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.32.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.32.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.32.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.32.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.32.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.32.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.32.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.33.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.33.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.33.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.33.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.33.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.33.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.33.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.33.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.33.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.33.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.33.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.33.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.34.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.34.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.34.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.34.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.34.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.34.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.34.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.34.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.34.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.34.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.34.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.34.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.35.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.35.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.35.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.35.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.35.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.35.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.35.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.35.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.35.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.35.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.35.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.35.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.36.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.36.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.36.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.36.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.36.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.36.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.36.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.36.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.36.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.36.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.36.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.36.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.37.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.37.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.37.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.37.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.37.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.37.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.37.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.37.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.37.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.37.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.37.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.37.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.38.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.38.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.38.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.38.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.38.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.38.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.38.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.38.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.38.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.38.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.38.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.38.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.39.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.39.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.39.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.39.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.39.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.39.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.39.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.39.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.39.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.39.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.39.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.39.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.40.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.40.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.40.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.40.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.40.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.40.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.40.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.40.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.40.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.40.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.40.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.40.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.41.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.41.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.41.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.41.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.41.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.41.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.41.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.41.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.41.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.41.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.41.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.41.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.42.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.42.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.42.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.42.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.42.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.42.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.42.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.42.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.42.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.42.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.42.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.42.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.43.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.43.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.43.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.43.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.43.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.43.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.43.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.43.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.43.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.43.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.43.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.43.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.44.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.44.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.44.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.44.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.44.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.44.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.44.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.44.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.44.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.44.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.44.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.44.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.45.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.45.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.45.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.45.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.45.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.45.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.45.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.45.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.45.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.45.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.45.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.45.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.46.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.46.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.46.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.46.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.46.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.46.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.46.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.46.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.46.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.46.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.46.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.46.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.47.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.47.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.47.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.47.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.47.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.47.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.47.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.47.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.47.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.47.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.47.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.47.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.48.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.48.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.48.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.48.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.48.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.48.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.48.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.48.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.48.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.48.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.48.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.48.input_layernorm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.49.self_attn.scaling torch.Size([80]) True\n",
            "model.stacked_transformer.layers.49.self_attn.qkv_proj.weight torch.Size([3840, 1280]) True\n",
            "model.stacked_transformer.layers.49.self_attn.qkv_proj.bias torch.Size([3840]) True\n",
            "model.stacked_transformer.layers.49.self_attn.o_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.49.self_attn.o_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.49.mlp.gate_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.49.mlp.gate_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.49.mlp.down_proj.weight torch.Size([1280, 1280]) True\n",
            "model.stacked_transformer.layers.49.mlp.down_proj.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.49.mlp.layer_norm.weight torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.49.mlp.layer_norm.bias torch.Size([1280]) True\n",
            "model.stacked_transformer.layers.49.input_layernorm.weight torch.Size([1280]) True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_timesfm_model(context_len, horizon_len):\n",
        "    return TimesFmWrapper(context_len=context_len, horizon_len=horizon_len)\n",
        "\n",
        "\n",
        "def run_finetuning(X_series, y_series):\n",
        "    context_len = X_series.shape[1]\n",
        "\n",
        "    # Dataset\n",
        "    dataset = GridTimeSeriesDataset(X_series, y_series)\n",
        "    train_size = int(len(dataset) * 0.8)\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Model and Config\n",
        "    model = load_timesfm_model(context_len,horizon_len)\n",
        "    config = FinetuningConfig(\n",
        "        batch_size=64,\n",
        "        num_epochs=2,\n",
        "        learning_rate=1e-4,\n",
        "        freq_type=0,\n",
        "        use_quantile_loss=False,\n",
        "        use_wandb=False,\n",
        "    )\n",
        "\n",
        "    # Finetune\n",
        "    trainer = TimesFMFinetuner(model, config)\n",
        "    trainer.finetune(train_dataset=train_dataset, val_dataset=val_dataset)\n"
      ],
      "metadata": {
        "id": "dqA7L1hcb5km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes: (86400, 264)\n",
        "# X_grid.shape # from HadGEM DSL\n",
        "y_grid.shape  # from AVISO DSL\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19_-fH-ZFoto",
        "outputId": "b386cfac-feee-40d6-8ae3-3643b73db5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(86400, 264)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you already ran this:\n",
        "# X_grid: (86400, 264) → input from HadGEM DSL\n",
        "# y_grid: (86400, 264) → target from AVISO DSL\n",
        "\n",
        "run_finetuning(X_grid, y_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "d34def2357114d58be577bd1549b2709",
            "88de117274c547b3ab9ec8eef7cf5231",
            "22ca31b4a75c4aa59dd2f6cafd376c4c",
            "b7d7c6262a174f398587546dec7aabc6",
            "a72891d240c8412db5fa1bf80dcebe90",
            "51ae2d462bfe4a1190dab0efcb6dcba4",
            "b7dc6425e0ef4a7199a1f4fe03a37020",
            "d1b5b0a519ce47c3ae29af6d14bb30bd",
            "fa16c483034046108784262c3c480a79",
            "0ba46fdeaf8e4263892fbb99d1e987fa",
            "f2acd852ead54f1fbfa79b68b241a922"
          ]
        },
        "id": "A0IQwgIBb5iC",
        "outputId": "79e4fa6f-cec5-4de4-b957-9a1fecda4b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d34def2357114d58be577bd1549b2709"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[64, -1, 32]' is invalid for input of size 16896",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-3476832623.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# y_grid: (86400, 264) → target from AVISO DSL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-30-3925369605.py\u001b[0m in \u001b[0;36mrun_finetuning\u001b[0;34m(X_series, y_series)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Finetune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimesFMFinetuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-25-1792032156.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-25-1792032156.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_loader, optimizer)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-25-1792032156.py\u001b[0m in \u001b[0;36m_process_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    262\u001b[0m     ]\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_padding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mpredictions_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mlast_patch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26-1277366710.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_context, x_padding, freq)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_padding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_padding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timesfm/pytorch_patched_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ts, input_padding, freq)\u001b[0m\n\u001b[1;32m    713\u001b[0m   ) -> torch.Tensor:\n\u001b[1;32m    714\u001b[0m     \u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m     model_input, patched_padding, stats, _ = self._preprocess_input(\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0minput_ts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0minput_padding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_padding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timesfm/pytorch_patched_decoder.py\u001b[0m in \u001b[0;36m_preprocess_input\u001b[0;34m(self, input_ts, input_padding)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;31m# Reshape into patches (using view for efficiency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0mbsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m     \u001b[0mpatched_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0mpatched_pads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_padding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, -1, 32]' is invalid for input of size 16896"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAeA2-GCdhMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I3nadEihJz5"
      },
      "source": [
        "# Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvOuCshmgbBY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQOW1A7iga-p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-uXCObkga8B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uptte5yda-l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUJ6Oq8qda7t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq_aPLtPh4S1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b231c95e262f491da36bc757fbc2532a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd7d2988ae214405aac81065a4a16b02",
              "IPY_MODEL_2cac7065229b4cd696bc8e8dfa19203c",
              "IPY_MODEL_120bf5a215544418b27865ba6bd7695a"
            ],
            "layout": "IPY_MODEL_10c7a0c83f7a47b281f98ac22bca25e4"
          }
        },
        "fd7d2988ae214405aac81065a4a16b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e363c282fbc945a6b793aa2f4373af07",
            "placeholder": "​",
            "style": "IPY_MODEL_1b34ffa2fe524a649ab9b1d09adc76ea",
            "value": "Fetching 5 files: 100%"
          }
        },
        "2cac7065229b4cd696bc8e8dfa19203c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1795320317a742b5a50f8ad6d411682e",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfb98964dea1486aad12427f66075df0",
            "value": 5
          }
        },
        "120bf5a215544418b27865ba6bd7695a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a1ca5dd5f65435e9cacc2419f0bd0f4",
            "placeholder": "​",
            "style": "IPY_MODEL_8908bcc711c84c5d956225442af81fa9",
            "value": " 5/5 [00:00&lt;00:00, 186.18it/s]"
          }
        },
        "10c7a0c83f7a47b281f98ac22bca25e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e363c282fbc945a6b793aa2f4373af07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b34ffa2fe524a649ab9b1d09adc76ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1795320317a742b5a50f8ad6d411682e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb98964dea1486aad12427f66075df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a1ca5dd5f65435e9cacc2419f0bd0f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8908bcc711c84c5d956225442af81fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d34def2357114d58be577bd1549b2709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88de117274c547b3ab9ec8eef7cf5231",
              "IPY_MODEL_22ca31b4a75c4aa59dd2f6cafd376c4c",
              "IPY_MODEL_b7d7c6262a174f398587546dec7aabc6"
            ],
            "layout": "IPY_MODEL_a72891d240c8412db5fa1bf80dcebe90"
          }
        },
        "88de117274c547b3ab9ec8eef7cf5231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51ae2d462bfe4a1190dab0efcb6dcba4",
            "placeholder": "​",
            "style": "IPY_MODEL_b7dc6425e0ef4a7199a1f4fe03a37020",
            "value": "Fetching 5 files: 100%"
          }
        },
        "22ca31b4a75c4aa59dd2f6cafd376c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b5b0a519ce47c3ae29af6d14bb30bd",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa16c483034046108784262c3c480a79",
            "value": 5
          }
        },
        "b7d7c6262a174f398587546dec7aabc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ba46fdeaf8e4263892fbb99d1e987fa",
            "placeholder": "​",
            "style": "IPY_MODEL_f2acd852ead54f1fbfa79b68b241a922",
            "value": " 5/5 [00:00&lt;00:00, 236.82it/s]"
          }
        },
        "a72891d240c8412db5fa1bf80dcebe90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ae2d462bfe4a1190dab0efcb6dcba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7dc6425e0ef4a7199a1f4fe03a37020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b5b0a519ce47c3ae29af6d14bb30bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa16c483034046108784262c3c480a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ba46fdeaf8e4263892fbb99d1e987fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2acd852ead54f1fbfa79b68b241a922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}